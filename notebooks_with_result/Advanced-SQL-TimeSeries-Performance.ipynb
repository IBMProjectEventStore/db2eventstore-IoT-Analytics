{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Benchmarking IBM Db2 Event Store's Time Series Performance in Consecutive Duplication Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Abstract\n",
    "This notebook contains a simple benchmark comparison in the performance of consecutive duplication removal between IBM Event Store's time series function and naive approach.\n",
    "\n",
    "## Procedure\n",
    "- A simple table is created with records containing a monotonically increasing timestamp. \n",
    "- Dataframe is created, repartitioned, and cached. Same cached dataframe is used both in Event Store time series and naive approach so that to eliminate the effect of data fetching time on the time series query performance measurement.\n",
    "- Consecutive duplication removal performance is measured with IBM Db2 Event Store's time series function\n",
    "    - Performance is first measured when time series is created in years.\n",
    "    - Performance is then measured when the time series is created in dates.\n",
    "- Consecutive duplication removal performance is measured with the naive approach.\n",
    "- Comparison and interpretation are concluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'app-20190926164629-0053'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show current Spark application ID\n",
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from eventstore.oltp import EventContext\n",
    "from eventstore.sql import EventSession\n",
    "from eventstore.common import ConfigurationReader\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ConfigurationReader.setEventUser(\"admin\")\n",
    "ConfigurationReader.setEventPassword(\"password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"EventStore SQL in Python\").getOrCreate()\n",
    "eventSession = EventSession(sparkSession.sparkContext, \"EVENTDB\")\n",
    "eventSession.set_query_read_option(\"SnapshotNow\")\n",
    "eventSession._jvm.org.apache.spark.sql.types.SqlTimeSeries.register(eventSession._jsparkSession)\n",
    "eventSession.open_database()\n",
    "ctx = EventContext.get_event_context(\"EVENTDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from eventstore.catalog import TableSchema\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part 1: Function preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def duplication_stats(L0_sdf, es_session):\n",
    "    \"\"\"\n",
    "    This function is rewritten based on the naive_on_change. The function will calculate and print follwoing info:\\\n",
    "    - total number of rows\n",
    "    - total number of duplications (count of record with duplicated readings in consecutive timestamps\n",
    "      , except for the ealirst record in each consecutive duplicated records)\n",
    "    - percentage of duplicated records to be removed\n",
    "    - Expected number of rows/records after duplication removal\n",
    "    ---\n",
    "    @param: L0_sdf: Spark Dataframe : Dataframe whose duplication statistics will be calculated\n",
    "    @param: es_session: EventSession to be used\n",
    "    @return: int : expected row count after duplication removal\n",
    "    ---\n",
    "    Example:\n",
    "    # duplication stats should be same for granularity of years/dates\n",
    "    duplication_stats(raw_table_with_dates, eventSession)\n",
    "    Return:\n",
    "            Total number of rows: 9999990\n",
    "            Total number of duplications: 999502\n",
    "            Duplication percentage: 0.09995029995029996\n",
    "            Expected row number after processing: 9000488\n",
    "    \"\"\"\n",
    "    from pyspark.sql import Window\n",
    "    from pyspark.sql.functions import lag\n",
    "    \n",
    "    # We will use current time to build the temp views names\n",
    "    def tabletag():\n",
    "        from time import time\n",
    "        return 'TABLE'+str(int(time()*1000000))\n",
    "    \n",
    "    # Time sort\n",
    "    L1_sdf = L0_sdf.orderBy('timestamp')\n",
    "    L0_sortedTable = tabletag()\n",
    "    L1_sdf.createOrReplaceTempView(L0_sortedTable)\n",
    "    \n",
    "    # Keep first record\n",
    "    L1_first_sdf = es_session.sql('SELECT timestamp, value FROM ' + L0_sortedTable + ' LIMIT 1')\n",
    "    \n",
    "    # Prepare lag program\n",
    "    eng_col = L1_sdf['value']\n",
    "    lag_eng = lag(eng_col).over(Window.orderBy('timestamp'))\n",
    "    L1_sdf = L1_sdf.withColumn('prev_value', lag_eng)\n",
    "    \n",
    "    # Prepare diff program\n",
    "    prev_eng_col = L1_sdf['prev_value']\n",
    "    L1_sdf = L1_sdf.withColumn('diff_value', eng_col - prev_eng_col)\n",
    "    \n",
    "    # Prepare on-change filter program\n",
    "    diff_eng_col = L1_sdf['diff_value']    \n",
    "    L1_count = L1_sdf.count()\n",
    "    L1_dup_count = L1_sdf.filter(diff_eng_col == 0).count()\n",
    "    L1_dup_percent = L1_dup_count / L1_count\n",
    "    L1_unique_count= L1_count - L1_dup_count\n",
    "    print(\"Total number of rows: {}\".format(L1_count))\n",
    "    print(\"Total number of duplications: {}\".format(L1_dup_count))\n",
    "    print(\"Duplication percentage: {}\".format(L1_dup_percent))\n",
    "    print(\"Expected row number after processing: {}\".format(L1_unique_count))\n",
    "    return L1_unique_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part 2: Data preparation\n",
    "As a proof-of-concept, a simple data is created with 10Million randomly generated data.  \n",
    "```\n",
    "root\n",
    " |-- KEY: integer (nullable = false)\n",
    " |-- TIMESTAMP: long (nullable = false)\n",
    " |-- VALUE: float (nullable = false)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_records = 10000000\n",
    "table_name = \"t\" + str(num_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTable creation/ data loading are commented out after the first run.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Table creation/ data loading are commented out after the first run.\n",
    "\"\"\"\n",
    "# # Define table schema to be created\n",
    "# with EventContext.get_event_context(\"EVENTDB\") as ctx:\n",
    "#     schema = StructType([\n",
    "#         StructField(\"key\", IntegerType(), nullable = False),\n",
    "#         StructField(\"timestamp\", LongType(), nullable = False),\n",
    "#         StructField(\"value\", FloatType(), nullable = False)\n",
    "#     ])  \n",
    "#     table_schema = TableSchema(table_name, schema,\n",
    "#                                 sharding_columns=[\"key\"],\n",
    "#                                 pk_columns=[\"key\",\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     ctx.create_table(table_schema)\n",
    "# except Exception as error:\n",
    "#     print(error)\n",
    "#     pass\n",
    "    \n",
    "# table_names = ctx.get_names_of_tables()\n",
    "# for idx, name in enumerate(table_names):\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# table = eventSession.load_event_table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ingest data into table\n",
    "# import os\n",
    "# resolved_table_schema = ctx.get_table(table_name)\n",
    "# print(resolved_table_schema)\n",
    "# for letter in list([\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\"]):\n",
    "#     with open(os.environ['DSX_PROJECT_DIR']+'/datasets/csv_10000000_realtime_xa'+letter+'.csv') as f:\n",
    "#         f.readline()\n",
    "#         content = f.readlines()\n",
    "#         content = [l.split(\",\") for l in content]\n",
    "#         batch = [dict(key=int(c[0]), timestamp=int(c[1]), value=float(c[2])) for c in content]\n",
    "#         ctx.batch_insert(resolved_table_schema, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1 Optimize parallelism by repartitioning\n",
    "Note that when the query is pushed down to Db2 Event Store and the data is retrieved, the data will be received by Spark as one single partitioned data frame. It's necessary for the user to explicitly repartition the dataframe.  \n",
    "It's suggested that one partition is created for each CPU core in the Spark cluster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions prior to time series (after loading table):  1\n",
      "partition sizes prior to time series (after loading table):  [9999990]\n"
     ]
    }
   ],
   "source": [
    "# verify ingested result\n",
    "raw_table = eventSession.load_event_table(table_name)\n",
    "\n",
    "print(\"number of partitions prior to time series (after loading table): \",raw_table.rdd.getNumPartitions())\n",
    "print(\"partition sizes prior to time series (after loading table): \", raw_table.rdd.mapPartitions(lambda s: iter([sum(1 for _ in s)])).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions prior to time series (after loading table):  48\n",
      "partition sizes prior to time series (after loading table):  [208333, 208334, 208334, 208334, 208334, 208334, 208334, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333, 208333]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "repartition the dataframe into 48 partitions (16 cores/node * 3 nodes = 48 partitions)\n",
    "\"\"\"\n",
    "raw_table_after_partition = raw_table.repartition(48)\n",
    "\n",
    "print(\"number of partitions prior to time series (after loading table): \",raw_table_after_partition.rdd.getNumPartitions())\n",
    "print(\"partition sizes prior to time series (after loading table): \", raw_table_after_partition.rdd.mapPartitions(lambda s: iter([sum(1 for _ in s)])).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "raw_table_after_partition.createOrReplaceTempView(\"raw_table_partitioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2 Generating new clustering key for time series creation\n",
    "\n",
    "Records will be clustered into certain ranges, such as years or dates, and time series will be created on each such clustered ranges or record.  \n",
    "Consecutive duplication removal will happen on each time series, which dramatically increase the computational parallelism and reduces computation time.  \n",
    "\n",
    "There are some subtle differences that worth noticing in the clustering granularity:\n",
    "\n",
    "1/ **Performance**\n",
    "\n",
    "In general, performance will increase with smaller clustering granularity when clustering consecutive record to create time series.\n",
    "\n",
    "When the records are clustered in smaller granularity, i.e. dates v.s. years, the number of time series created will be increased.  \n",
    "Duplication removal will be executed concurrently on all-time series, thus the performance is better.  \n",
    "  \n",
    "2/ **Number of remaining duplications**\n",
    "\n",
    "With smaller clustering granularity, more time series will be created, and the number of duplications left over will increase.\n",
    "\n",
    "For example, that grouping by key [day 1] [day 2] … [day n]. If you remove dups on each one, If let's say the last value of day 1 is dup with the first   value of day 2, it will not catch that as dups were removed on a each-time-series basis. Having said that, on a large scale, this will always occur as it   depends on how much data you are querying each time, for instance, if they query 1 day at a time, unless they keep track of the last value in each day every   time they query, for the next time they query, they will also have this issue.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "Two dataframes are created with different clustering granularity for performance comparsion:\n",
    "\n",
    "- raw_table_with_years: Clustering key in years\n",
    "\n",
    "- raw_table_with_dates: Clustering key in dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ts granularity in years\n",
    "raw_table_with_years = eventSession.sql(\"select key, from_unixtime(TIMESTAMP/1000,'YYYY') as key2, TIMESTAMP, value from raw_table_partitioned\").cache()\n",
    "# ts granularity in dates\n",
    "raw_table_with_dates = eventSession.sql(\"select key, from_unixtime(TIMESTAMP/1000,'YYYY-MM-dd') as key2, TIMESTAMP, value from raw_table_partitioned\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+-----+\n",
      "|key|      key2|    TIMESTAMP|value|\n",
      "+---+----------+-------------+-----+\n",
      "|  1|2019-09-18|1568828943497|  6.0|\n",
      "|  1|2019-09-18|1568831823497|  4.0|\n",
      "|  1|2019-09-18|1568834703497|  3.0|\n",
      "|  1|2019-09-18|1568837583497|  8.0|\n",
      "|  1|2019-09-18|1568840463497|  6.0|\n",
      "+---+----------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_table_with_dates.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+-----+\n",
      "|key|key2|    TIMESTAMP|value|\n",
      "+---+----+-------------+-----+\n",
      "|  1|2019|1568828943497|  6.0|\n",
      "|  1|2019|1568831823497|  4.0|\n",
      "|  1|2019|1568834703497|  3.0|\n",
      "|  1|2019|1568837583497|  8.0|\n",
      "|  1|2019|1568840463497|  6.0|\n",
      "+---+----+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_table_with_years.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Show the range of the timestamp**\n",
    "\n",
    "- Notice that the time spans 19 years, representing 7045 days.\n",
    "- Notice that the distinct dates are 6929, which means we have records for almost all dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(max(key2)='2038-12-31')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_table_with_dates.agg({\"key2\":\"max\"}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(min(key2)='2019-09-18')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_table_with_dates.agg({\"key2\":\"min\"}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6929"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_table_with_dates.select(\"key2\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 9999990\n",
      "Total number of duplications: 999502\n",
      "Duplication percentage: 0.09995029995029996\n",
      "Expected row number after processing: 9000488\n"
     ]
    }
   ],
   "source": [
    "# duplication stats should be same for granularity of years/dates\n",
    "\"\"\"\n",
    "Note that there are ~10% of duplicated records that's need to be removed.\n",
    "Expected total number after processing is : 900488\n",
    "\"\"\"\n",
    "expected_row_num = duplication_stats(raw_table_with_dates, eventSession)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3. Performance Analysis\n",
    "### 3.1 IBM Event Store Time Series Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create(df, key_col, ts_col, val_col, new_key_name=\"joined_primary_keys\", time_series_name=None):\n",
    "    \"\"\"\n",
    "    Highly efficient algorithm that creates a time series from the Spark Dataframe.\n",
    "    ---\n",
    "    @param df: Spark Dataframe : Containing input columns for time series creation\n",
    "    @key_col: List[String] : List of column name strings of primary key for the time series creation\n",
    "    @ts_col: String : Column name of timestamp\n",
    "    @val_col: String : Column name of value\n",
    "    @new_key_name: String : Column name of the joined primary key column to be created.\n",
    "    @time_series_name: String : [Default: <val_col>_time_series] Column name of the time series column to be created.\n",
    "    return: [Spark Dataframe] Spark df containing 2 columns: key column and time series column\n",
    "    ---\n",
    "    Example:\n",
    "    ts_df = create(raw_table_with_dates, [\"SATID\",\"PKID\",\"DATE\"], \"TIMESTAMP\", \"READING\")\n",
    "    \"\"\"\n",
    "    from pyspark.sql import DataFrame\n",
    "    from pyspark.sql.functions import concat, col, lit\n",
    "    ts_column_name = val_col + \"_time_series\"\n",
    "    df = df.withColumn(new_key_name, concat(*key_col))\n",
    "    ts_df = DataFrame(\n",
    "        df.sql_ctx._jvm.com.ibm.research.time_series.spark_timeseries_sql.utils.api.java.TimeSeriesDataFrame.create(\n",
    "            df._jdf,\n",
    "            new_key_name,\n",
    "            ts_col,\n",
    "            val_col\n",
    "        ),\n",
    "        df.sql_ctx\n",
    "    )\n",
    "    if time_series_name:\n",
    "        ts_df = ts_df.withColumnRenamed(ts_column_name, time_series_name)\n",
    "    return ts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1.1 Performance Comparision between SQL UDAF and create function\n",
    "\n",
    "1/ **Spark User Defined Aggregate Function (UDAF) : `TIME_SERIES`**\n",
    "\n",
    "Spark UDAF goes through each row of the dataframe, and creates a new time series by aggregating the previous rows and current row.\n",
    "Because the Spark RDD is immutable, multiple intermediate RDDs will be created.\n",
    "\n",
    "so for instance if you have 3 rows: [1] | [2] | [3], it will look as such [1] … [1] + [2] = [1,2] … [1,2] + [3] = [1,2,3]. Thus 5 intermediate rdds are created: [1] [2] [3] [1,2] [1,2,3].\n",
    "\n",
    "Example: \n",
    "```sql\n",
    "stmt = \"SELECT location, TIME_SERIES(timestamp, humidity) AS ts FROM dht_raw_table where humidity < 70 GROUP BY location\"\n",
    "```\n",
    "\n",
    "2/ **create function:**\n",
    "\n",
    "The create function simply group the given dataframe by key, and create one time series for each key at once. The performance advantage of the create function will be increasingly obvious with the larger dataframe and time series size.\n",
    "\n",
    "Example: \n",
    "```python\n",
    "create(dht_table, [\"location\"], \"timestamp\", \"HUMIDITY\", \"LOCATION\")\n",
    "```\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      key2|   value_time_series|\n",
      "+----------+--------------------+\n",
      "|2020-01-27|[(1580083203497,4...|\n",
      "|2020-02-17|[(1581897603497,3...|\n",
      "|2020-03-07|[(1583539203497,9...|\n",
      "|2020-10-03|[(1601683203497,4...|\n",
      "|2020-11-26|[(1606348803497,4...|\n",
      "|2020-12-16|[(1608076803497,7...|\n",
      "|2021-01-26|[(1611619203497,0...|\n",
      "|2021-02-16|[(1613433603497,8...|\n",
      "|2021-03-06|[(1614988803497,1...|\n",
      "|2021-04-29|[(1619654403497,9...|\n",
      "|2021-05-19|[(1621382403497,4...|\n",
      "|2021-06-09|[(1623196803497,6...|\n",
      "|2021-10-02|[(1633132803497,1...|\n",
      "|2021-11-25|[(1637798403497,4...|\n",
      "|2021-12-15|[(1639526403497,3...|\n",
      "|2022-01-25|[(1643068803497,2...|\n",
      "|2022-02-15|[(1644883203497,4...|\n",
      "|2022-03-05|[(1646438403497,6...|\n",
      "|2022-04-28|[(1651104003497,6...|\n",
      "|2022-05-18|[(1652832003497,0...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 9.43 ms, sys: 18.2 ms, total: 27.6 ms\n",
      "Wall time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "creating ts using create function\n",
    "'''\n",
    "create(raw_table_with_dates, [\"key2\"], \"timestamp\", \"value\", new_key_name=\"key2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "raw_table_with_dates.createOrReplaceTempView(\"raw_table_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      key2|                  ts|\n",
      "+----------+--------------------+\n",
      "|2020-02-26|[(1582675203497,0...|\n",
      "|2020-04-13|[(1586736003497,6...|\n",
      "|2021-11-03|[(1635897603497,8...|\n",
      "|2022-10-05|[(1664928003497,3...|\n",
      "|2023-01-21|[(1674259203497,7...|\n",
      "|2023-05-01|[(1682899203497,0...|\n",
      "|2023-05-18|[(1684368003497,3...|\n",
      "|2024-01-19|[(1705622403497,8...|\n",
      "|2024-07-14|[(1720915203497,2...|\n",
      "|2024-08-20|[(1724112003497,4...|\n",
      "|2024-09-15|[(1726358403497,7...|\n",
      "|2024-10-24|[(1729728003497,5...|\n",
      "|2026-02-01|[(1769904003497,7...|\n",
      "|2026-10-14|[(1791936003497,2...|\n",
      "|2027-07-13|[(1815436803497,9...|\n",
      "|2028-08-15|[(1849910403497,0...|\n",
      "|2028-08-16|[(1849996803497,6...|\n",
      "|2029-09-26|[(1885075203497,1...|\n",
      "|2031-03-20|[(1931731203497,3...|\n",
      "|2032-09-01|[(1977609603497,8...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 16.8 ms, sys: 8.79 ms, total: 25.6 ms\n",
      "Wall time: 43.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "creating ts using Spark UDAF\n",
    "'''\n",
    "\n",
    "stmt = \"\"\"SELECT key2, TIME_SERIES(timestamp, value) AS ts FROM raw_table_partitioned GROUP BY key2\"\"\"\n",
    "\n",
    "eventSession.sql(stmt).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Processing Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Case 1: TS granularity in years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-----+\n",
      "|key2|    time_tick|value|\n",
      "+----+-------------+-----+\n",
      "|2019|1568826123497|  1.0|\n",
      "|2019|1568826183497|  2.0|\n",
      "|2019|1568826243497|  4.0|\n",
      "|2019|1568826303497|  5.0|\n",
      "|2019|1568826363497|  2.0|\n",
      "|2019|1568826423497|  0.0|\n",
      "|2019|1568826483497|  5.0|\n",
      "|2019|1568826543497|  3.0|\n",
      "|2019|1568826603497|  5.0|\n",
      "|2019|1568826663497|  6.0|\n",
      "|2019|1568826723497|  9.0|\n",
      "|2019|1568826783497|  8.0|\n",
      "|2019|1568826843497|  2.0|\n",
      "|2019|1568826903497|  4.0|\n",
      "|2019|1568826963497|  9.0|\n",
      "|2019|1568827023497|  5.0|\n",
      "|2019|1568827083497|  4.0|\n",
      "|2019|1568827143497|  2.0|\n",
      "|2019|1568827263497|  1.0|\n",
      "|2019|1568827323497|  8.0|\n",
      "+----+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total processing time is  111.08714270591736 seconds\n",
      "CPU times: user 44.4 ms, sys: 14 ms, total: 58.4 ms\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "ts_df = create(raw_table_with_years, [\"key2\"], \"timestamp\", \"value\", new_key_name=\"key2\")\n",
    "ts_df.createOrReplaceTempView(\"ts_table\")\n",
    "# force execution\n",
    "ts_df_unique = eventSession.sql(\"select key2, ts_explode(ts_remove_consecutive_duplicates(value_time_series)) as (time_tick, value) from ts_table\")\n",
    "ts_df_unique.show()\n",
    "end = time.time()\n",
    "print(\"Total processing time is \",end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count after process:  9000488\n"
     ]
    }
   ],
   "source": [
    "row_count_after_process = ts_df_unique.count()\n",
    "print(\"Row count after process: \", row_count_after_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated record remaining after processing: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of duplicated record remaining after processing: {}\".format(row_count_after_process- expected_row_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions after time series creation:  48\n",
      "partition sizes after time series creation:  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "temp_ts_df = eventSession.sql(\"select key2, ts_count(value_time_series) as c from ts_table\")\n",
    "print(\"number of partitions after time series creation: \",temp_ts_df.rdd.getNumPartitions())\n",
    "print(\"partition sizes after time series creation: \", temp_ts_df.rdd.mapPartitions(lambda s: iter([sum(1 for _ in s)])).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Case 2: TS granularity in dates\n",
    "\n",
    "When compared with the granularity in years:\n",
    "\n",
    "**Pros:** Reduce the granularity to date will increase the number of time series to be created, leading to better parallelism.\n",
    "\n",
    "**Cons:** Increasing the number of time series will also increase the number of duplications that will retain after duplication removal: when grouping by key [day 1] [day 2] … [day n]. If you remove dups on each one, If let's say the last value of day 1 is dup with the first value of day 2, it will not catch that as dups were removed on a each-time-series basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----+\n",
      "|      key2|    time_tick|value|\n",
      "+----------+-------------+-----+\n",
      "|2020-01-27|1580083203497|  4.0|\n",
      "|2020-01-27|1580083263497|  8.0|\n",
      "|2020-01-27|1580083383497|  3.0|\n",
      "|2020-01-27|1580083443497|  9.0|\n",
      "|2020-01-27|1580083503497|  2.0|\n",
      "|2020-01-27|1580083563497|  1.0|\n",
      "|2020-01-27|1580083623497|  3.0|\n",
      "|2020-01-27|1580083683497|  6.0|\n",
      "|2020-01-27|1580083743497|  1.0|\n",
      "|2020-01-27|1580083803497|  0.0|\n",
      "|2020-01-27|1580083863497|  1.0|\n",
      "|2020-01-27|1580083923497|  3.0|\n",
      "|2020-01-27|1580083983497|  5.0|\n",
      "|2020-01-27|1580084103497|  8.0|\n",
      "|2020-01-27|1580084163497|  5.0|\n",
      "|2020-01-27|1580084223497|  8.0|\n",
      "|2020-01-27|1580084283497|  6.0|\n",
      "|2020-01-27|1580084403497|  2.0|\n",
      "|2020-01-27|1580084463497|  0.0|\n",
      "|2020-01-27|1580084523497|  4.0|\n",
      "+----------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total processing time:  35.89651012420654 seconds\n",
      "CPU times: user 14.6 ms, sys: 4.23 ms, total: 18.8 ms\n",
      "Wall time: 35.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "ts_df = create(raw_table_with_dates, [\"key2\"], \"timestamp\", \"value\",\"key2\")\n",
    "ts_df.createOrReplaceTempView(\"ts_table\")\n",
    "ts_df_unique = eventSession.sql(\"select key2, ts_explode(ts_remove_consecutive_duplicates(value_time_series)) as (time_tick, value) from ts_table\")\n",
    "ts_df_unique.show()\n",
    "end = time.time()\n",
    "print(\"Total processing time: \", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      key2|   value_time_series|\n",
      "+----------+--------------------+\n",
      "|2020-01-27|[(1580083203497,4...|\n",
      "|2020-02-17|[(1581897603497,3...|\n",
      "|2020-03-07|[(1583539203497,9...|\n",
      "|2020-10-03|[(1601683203497,4...|\n",
      "|2020-11-26|[(1606348803497,4...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count after process:  9001185\n"
     ]
    }
   ],
   "source": [
    "row_count_after_process = ts_df_unique.count()\n",
    "print(\"Row count after process: \", row_count_after_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated record remaining after processing: 697\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of duplicated record remaining after processing: {}\".format(row_count_after_process- expected_row_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions after time series creation:  48\n",
      "partition sizes after time series creation:  [165, 169, 162, 146, 137, 127, 127, 127, 126, 126, 125, 137, 153, 163, 165, 168, 161, 154, 163, 158, 146, 133, 115, 116, 119, 131, 139, 137, 137, 156, 169, 169, 169, 162, 151, 148, 143, 139, 127, 118, 119, 121, 132, 149, 154, 153, 156, 162]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Partition distribution of time series created.\n",
    "\"\"\"\n",
    "temp_ts_df = eventSession.sql(\"select key2, ts_count(value_time_series) as c from ts_table\")\n",
    "print(\"number of partitions after time series creation: \",temp_ts_df.rdd.getNumPartitions())\n",
    "print(\"partition sizes after time series creation: \", temp_ts_df.rdd.mapPartitions(lambda s: iter([sum(1 for _ in s)])).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.2 Naive approach performance\n",
    "\n",
    "In the naive approach, we compare the reading at the current timestamp versus the reading at the previous timestamp.\n",
    "If there is are consecutive duplications, only the earliest reading will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def naive_on_change(L0_sdf, es_session):\n",
    "    from pyspark.sql import Window\n",
    "    from pyspark.sql.functions import lag\n",
    "    \n",
    "    # We will use current time to build the temp views names\n",
    "    def tabletag():\n",
    "        from time import time\n",
    "        return 'TABLE'+str(int(time()*1000000))\n",
    "    \n",
    "    # Time sort\n",
    "    L1_sdf = L0_sdf.orderBy('TIMESTAMP')\n",
    "    L0_sortedTable = tabletag()\n",
    "    L1_sdf.createOrReplaceTempView(L0_sortedTable)\n",
    "    \n",
    "    # Keep first record\n",
    "    L1_first_sdf = es_session.sql('SELECT TIMESTAMP, value FROM ' + L0_sortedTable + ' LIMIT 1')\n",
    "    \n",
    "    # Prepare lag program, for record at each timestamp, add a column of readings of the previous timestamp\n",
    "    eng_col = L1_sdf['value']\n",
    "    lag_eng = lag(eng_col).over(Window.orderBy('TIMESTAMP'))\n",
    "    L1_sdf = L1_sdf.withColumn('prev_value', lag_eng)\n",
    "    \n",
    "    # Prepare diff program, compare reading(now_timestamp) v.s reading(previous_timestamp)\n",
    "    prev_eng_col = L1_sdf['prev_value']\n",
    "    L1_sdf = L1_sdf.withColumn('diff_value', eng_col - prev_eng_col)\n",
    "    \n",
    "    # Prepare on-change filter program, drop record row if reading(now_timestamp) is identical with the reading(previous_timestamp)\n",
    "    diff_eng_col = L1_sdf['diff_value']\n",
    "    L1_sdf = L1_sdf.filter(diff_eng_col != 0)\n",
    "    \n",
    "    # Remove intermediate computing columns, only keep the earlist record with the smallest timestamp\n",
    "    # for duplicated consecutive records.\n",
    "    L1_sdf = L1_sdf.select('TIMESTAMP', 'value')\n",
    "    \n",
    "    # Append first record\n",
    "    L1_sdf = L1_first_sdf.union(L1_sdf).distinct().orderBy('TIMESTAMP')\n",
    "    \n",
    "    # Return result\n",
    "    return L1_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|    TIMESTAMP|value|\n",
      "+-------------+-----+\n",
      "|1568826123497|  1.0|\n",
      "|1568826183497|  2.0|\n",
      "|1568826243497|  4.0|\n",
      "|1568826303497|  5.0|\n",
      "|1568826363497|  2.0|\n",
      "|1568826423497|  0.0|\n",
      "|1568826483497|  5.0|\n",
      "|1568826543497|  3.0|\n",
      "|1568826603497|  5.0|\n",
      "|1568826663497|  6.0|\n",
      "|1568826723497|  9.0|\n",
      "|1568826783497|  8.0|\n",
      "|1568826843497|  2.0|\n",
      "|1568826903497|  4.0|\n",
      "|1568826963497|  9.0|\n",
      "|1568827023497|  5.0|\n",
      "|1568827083497|  4.0|\n",
      "|1568827143497|  2.0|\n",
      "|1568827263497|  1.0|\n",
      "|1568827323497|  8.0|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "58.79403209686279 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = naive_on_change(raw_table_with_dates,eventSession)\n",
    "df.show()\n",
    "end = time.time()\n",
    "print(end - start, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Sort [TIMESTAMP#1L ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(TIMESTAMP#1L ASC NULLS FIRST, 200)\n",
      "   +- *HashAggregate(keys=[TIMESTAMP#1L, value#2], functions=[])\n",
      "      +- Exchange hashpartitioning(TIMESTAMP#1L, value#2, 200)\n",
      "         +- *HashAggregate(keys=[TIMESTAMP#1L, value#2], functions=[])\n",
      "            +- Union\n",
      "               :- TakeOrderedAndProject(limit=1, orderBy=[TIMESTAMP#1L ASC NULLS FIRST], output=[TIMESTAMP#1L,value#2])\n",
      "               :  +- InMemoryTableScan [TIMESTAMP#1L, value#2]\n",
      "               :        +- InMemoryRelation [key#0, key2#35, TIMESTAMP#1L, value#2], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :              +- *Project [key#0, from_unixtime(cast((cast(TIMESTAMP#1L as double) / 1000.0) as bigint), YYYY-MM-dd, Some(Etc/UTC)) AS key2#35, TIMESTAMP#1L, value#2]\n",
      "               :                 +- Exchange RoundRobinPartitioning(48)\n",
      "               :                    +- Scan PushDownSource[KEY#0,TIMESTAMP#1L,VALUE#2]\n",
      "               +- *Project [TIMESTAMP#1L, value#2]\n",
      "                  +- *Filter (isnotnull(prev_value#689) && NOT ((value#2 - prev_value#689) = 0.0))\n",
      "                     +- Window [lag(value#2, 1, null) windowspecdefinition(TIMESTAMP#1L ASC NULLS FIRST, ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) AS prev_value#689], [TIMESTAMP#1L ASC NULLS FIRST]\n",
      "                        +- *Sort [TIMESTAMP#1L ASC NULLS FIRST], false, 0\n",
      "                           +- Exchange SinglePartition\n",
      "                              +- *Sort [TIMESTAMP#1L ASC NULLS FIRST], true, 0\n",
      "                                 +- Exchange rangepartitioning(TIMESTAMP#1L ASC NULLS FIRST, 200)\n",
      "                                    +- InMemoryTableScan [TIMESTAMP#1L, value#2]\n",
      "                                          +- InMemoryRelation [key#0, key2#35, TIMESTAMP#1L, value#2], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                                +- *Project [key#0, from_unixtime(cast((cast(TIMESTAMP#1L as double) / 1000.0) as bigint), YYYY-MM-dd, Some(Etc/UTC)) AS key2#35, TIMESTAMP#1L, value#2]\n",
      "                                                   +- Exchange RoundRobinPartitioning(48)\n",
      "                                                      +- Scan PushDownSource[KEY#0,TIMESTAMP#1L,VALUE#2]\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000488"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "source": [
    "## Conclusion:\n",
    "\n",
    "Total processing times are:\n",
    "- Time series approach\n",
    "    - granularity in years: \n",
    "        103.34106540679932  seconds\n",
    "    - granularity in dates:\n",
    "        78.09961080551147 seconds\n",
    "- Naive approach\n",
    "    126.62504267692566 seconds\n",
    "\n",
    "\n",
    "In general, the performance of consecutive duplication removal using IBM Db2 Event Store's Time Series approach is in linear order with that using the naive approach. There are, however, cases that IBM Db2 Event Store's Time Series function performs significantly better than the naive approach. Time series approaches also have the advantage of re-usability and flexible clustering key.\n",
    "\n",
    "**1/ Performance:**\n",
    "\n",
    "**1.1/ Time series creation using Spark UDAF SQL versus Create function**\n",
    "\n",
    "\n",
    "\n",
    "**1.2/ Consecutive duplication removal using Time Series approach versus Naive approach**\n",
    "\n",
    "The Time Series approach generally performs faster as the clustering granularity decreases when creating time series. For example, duplication removal performance on time series created per year is generally slower than the time series created per day. The reason is that more time series will be created with smaller cluster granularity, allowing for the duplication removal process to concurrently run on multiple time series. \n",
    "\n",
    "There is a small caveat, however, that the number of remaining duplications will increase with smaller clustering granularity. For example, that grouping by key [day 1] [day 2] … [day n]. If you remove dups on each one, If let's say the last value of day 1 is dup with the first value of day 2, it will not catch that as dups were removed on an each-time-series basis. Having said that, on a large scale, this will always occur as it depends on how much data you are querying each time, for instance, if they query 1 day at a time, unless they keep track of the last value in each day every time they query, for the next time they query, they will also have this issue.\n",
    "\n",
    "**2/ Reusability:**\n",
    "\n",
    "Intermediate data frame will be created containing time series. Those time series, which are compatible with other Event Store time series functions,  can be easily cached and re-used in future operations, whereas the naive approach will need manual manipulation.\n",
    "\n",
    "**3/ Flexible clustering key:**\n",
    "\n",
    "The create function provides a highly efficient way of creating time series using provided keys. It accepts multiple key columns.\n",
    "If user has a table:\n",
    "```\n",
    "root\n",
    " |-- DEVICEID: integer (nullable = false)\n",
    " |-- SENSORID: integer (nullable = false)\n",
    " |-- TIMESTAMP: long (nullable = false)\n",
    " |-- READING: double (nullable = false)\n",
    " ```\n",
    "User can chose to create time series on `key = [DEVICEID]` to eliminate consecutive duplications per device, or create time series on `key = [\"DEVICEID\", \"SENSORID\"]` to eliminate consecutive duplications per sensor on each device."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with Watson Studio Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
