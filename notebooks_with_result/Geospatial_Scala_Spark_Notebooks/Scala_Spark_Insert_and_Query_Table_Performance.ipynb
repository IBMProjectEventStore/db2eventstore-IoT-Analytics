{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"<AWS_S3_ACCESS>\")\n",
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"<AWS_S3_SECRET>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MIN_LAT = 30.0\n",
       "MAX_LAT = 50.0\n",
       "MIN_LON = -124.0\n",
       "MAX_LON = -74.0\n",
       "RES = 0.01\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val MIN_LAT = 30.0\n",
    "val MAX_LAT = 50.0\n",
    "val MIN_LON = -124.0\n",
    "val MAX_LON = -74.0\n",
    "val RES = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llat = ParallelCollectionRDD[35] at parallelize at <console>:33\n",
       "llon = ParallelCollectionRDD[36] at parallelize at <console>:34\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[36] at parallelize at <console>:34"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val llat = spark.sparkContext.parallelize(MIN_LAT until MAX_LAT by RES)\n",
    "val llon = spark.sparkContext.parallelize(MIN_LON until MAX_LON by RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [llat: double, llon: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[llat: double, llon: double ... 2 more fields]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = llat.cartesian(llon).toDF(\"llat\", \"llon\").withColumn(\"ulat\", col(\"llat\") + RES).withColumn(\"ulon\", col(\"llon\") + RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+-------------------+\n",
      "|llat|   llon| ulat|               ulon|\n",
      "+----+-------+-----+-------------------+\n",
      "|30.0| -124.0|30.01|            -123.99|\n",
      "|30.0|-123.99|30.01|-123.97999999999999|\n",
      "|30.0|-123.98|30.01|            -123.97|\n",
      "+----+-------+-----+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"s3a://vz-raster-images/performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.ibm.event.common.ConfigurationReader\n",
    "import com.ibm.event.oltp.{EventContext,InsertResult}\n",
    "import org.apache.spark.sql.ibm.event.EventSession\n",
    "import com.ibm.event.catalog.{TableSchema,IndexSpecification,SortSpecification,ColumnOrder}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.concurrent._\n",
    "import scala.concurrent.duration._\n",
    "import collection.JavaConverters._\n",
    "\n",
    "import com.ibm.research.st.algorithms.hashing.eg.GeoHashEG\n",
    "import com.ibm.research.st.datamodel.geometry.ellipsoidal.impl.BoundingBoxEG\n",
    "import com.ibm.research.st.util.BitVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Context Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ctx = com.ibm.event.oltp.EventContext@14c0ede\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "com.ibm.event.oltp.EventContext@14c0ede"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConfigurationReader.setDeploymentID(\"db2eventstore-1583771517259\")\n",
    "ConfigurationReader.setEventUser(\"<EVENTSTORE_USER_NAME>\")\n",
    "ConfigurationReader.setEventPassword(\"<EVENTSTORE_PASSWORD>\")\n",
    "ConfigurationReader.setConnectionEndpoints(\"172.30.176.26:18729;172.30.176.26:1100\")\n",
    "\n",
    "val ctx = EventContext.getEventContext(\"EVENTDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bitDepth = 29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bitDepth = 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tabName = performance\n",
       "tabSchema = TableSchema(tableName=performance, schemaName=Some(RGANTI), schema=StructType(StructField(llat,DoubleType,false), StructField(llon,DoubleType,false), StructField(ulat,DoubleType,false), StructField(ulon,DoubleType,false), StructField(id,LongType,false), StructField(geohash,LongType,false)), shardingColumns=WrappedArray(id), pkColumns=WrappedArray(id, geohash), partitionColumns=None)\n",
       "indexSpec = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "IndexSpecification(indexName=performanceIndex, indexID=None,\n",
       "equalColumns=(id),\n",
       "sortColumns=(SortSpecification(geohash,AscendingNullsLast)),\n",
       "includeColumns=())\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val tabName = \"performance\"\n",
    "val tabSchema = TableSchema(tabName, \n",
    "                            StructType(Array(\n",
    "                                StructField(\"llat\", DoubleType, nullable = false),\n",
    "                                StructField(\"llon\", DoubleType, nullable = false),\n",
    "                                StructField(\"ulat\", DoubleType, nullable = false),\n",
    "                                StructField(\"ulon\", DoubleType, nullable = false),\n",
    "                                StructField(\"id\", LongType, nullable = false),\n",
    "                                StructField(\"geohash\", LongType, nullable = false)\n",
    "                            )),\n",
    "                            shardingColumns = Array(\"id\"),\n",
    "                            pkColumns = Array(\"id\", \"geohash\")\n",
    "                           )\n",
    "val indexSpec = IndexSpecification(\n",
    "    indexName = tabName + \"Index\",\n",
    "    tableSchema = tabSchema,\n",
    "    equalColumns = Seq(\"id\"),\n",
    "    sortColumns = Seq(SortSpecification(\"geohash\", ColumnOrder.AscendingNullsLast))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop table if table already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt to drop table if table already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try {\n",
    "    println(\"Attempt to drop table if table already exists\")\n",
    "    ctx.dropTable(tabName)\n",
    "} catch {\n",
    "    case e: Exception =>\n",
    "    val TableNotFound = e.getMessage.split(\" \").contains(\"SQLCODE=-204,\")\n",
    "    if (TableNotFound) {\n",
    "        println(\"Table not found, skip dropping table...\")\n",
    "    } else {\n",
    "        println(\"EXCEPTION: Exception during drop table. Trying to exit...\" + e.getMessage)\n",
    "        e.printStackTrace()\n",
    "        sys.exit(1)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table performance\n",
      "Table schema = ResolvedTableSchema(tableName=RGANTIPERFORMANCE, schemaName=RGANTI, tableID=28, tableGroupName=sys_RGANTIPERFORMANCE, tableGroupID=14, numShareds=36, schema=StructType(StructField(LLAT,DoubleType,false), StructField(LLON,DoubleType,false), StructField(ULAT,DoubleType,false), StructField(ULON,DoubleType,false), StructField(ID,LongType,false), StructField(GEOHASH,LongType,false)),\n",
      "shardingColumns=(ID),\n",
      "pkColumns=(ID,GEOHASH),\n",
      "pkIndex=Some(IndexSpecification(indexName=__primaryKey, indexID=Some(0),\n",
      "equalColumns=(),\n",
      "sortColumns=(SortSpecification(ID,AscendingNullsLast),SortSpecification(GEOHASH,AscendingNullsLast)),\n",
      "includeColumns=())),\n",
      "indexes=(IndexSpecification(indexName=__primaryKey, indexID=Some(0),\n",
      "equalColumns=(),\n",
      "sortColumns=(SortSpecification(ID,AscendingNullsLast),SortSpecification(GEOHASH,AscendingNullsLast)),\n",
      "includeColumns=()))stringLengthsMap())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res = None\n",
       "tab = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ResolvedTableSchema(tableName=RGANTIPERFORMANCE, schemaName=RGANTI, tableID=28, tableGroupName=sys_RGANTIPERFORMANCE, tableGroupID=14, numShareds=36, schema=StructType(StructField(LLAT,DoubleType,false), StructField(LLON,DoubleType,false), StructField(ULAT,DoubleType,false), StructField(ULON,DoubleType,false), StructField(ID,LongType,false), StructField(GEOHASH,LongType,false)),\n",
       "shardingColumns=(ID),\n",
       "pkColumns=(ID,GEOHASH),\n",
       "pkIndex=Some(IndexSpecification(indexName=__primaryKey, indexID=Some(0),\n",
       "equalColumns=(),\n",
       "sortColumns=(SortSpecification(ID,AscendingNullsLast),SortSpecification(GEOHASH,AscendingNullsLast)),\n",
       "includeColumns=())),\n",
       "indexes=(IndexSpecification(indexName=__primaryKey, index...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(\"Creating table \" + tabName)\n",
    "var res = ctx.createTableWithIndex(tabSchema, indexSpec)\n",
    "assert(res.isEmpty, s\"create table: ${res.getOrElse(\"success\")}\")\n",
    "val tab = ctx.getTable(tabName)\n",
    "println(\"Table schema = \" + tab )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"<AWS_ACCESS_KEY>\")\n",
    "spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"<AWS_SECRET_KEY>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [llat: double, llon: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[llat: double, llon: double ... 2 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.parquet(\"s3a://vz-raster-images/performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- llat: double (nullable = true)\n",
      " |-- llon: double (nullable = true)\n",
      " |-- ulat: double (nullable = true)\n",
      " |-- ulon: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+------------------+\n",
      "|llat|llon  |ulat |ulon              |\n",
      "+----+------+-----+------------------+\n",
      "|30.0|-99.0 |30.01|-98.99            |\n",
      "|30.0|-98.99|30.01|-98.97999999999999|\n",
      "|30.0|-98.98|30.01|-98.97            |\n",
      "+----+------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geoHashUDF = UserDefinedFunction(<function4>,ArrayType(LongType,false),Some(List(DoubleType, DoubleType, DoubleType, DoubleType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function4>,ArrayType(LongType,false),Some(List(DoubleType, DoubleType, DoubleType, DoubleType)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val geoHashUDF = udf((llat: Double, llon: Double, ulat: Double, ulon: Double) => {\n",
    "    val bbox = new BoundingBoxEG(llat, llon, ulat, ulon)\n",
    "    val bvs = GeoHashEG.getInstance().geoHashCoverAtDepth(bbox, bitDepth).asScala\n",
    "    bvs.map(bv => (new BitVector(bv)).getLongArray()(0) >>> 1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+------------------+---+-------------------+\n",
      "|llat|llon  |ulat |ulon              |id |geohash            |\n",
      "+----+------+-----+------------------+---+-------------------+\n",
      "|30.0|-99.0 |30.01|-98.99            |0  |2838221454141554688|\n",
      "|30.0|-99.0 |30.01|-98.99            |0  |2838221471321423872|\n",
      "|30.0|-99.0 |30.01|-98.99            |0  |2838221488501293056|\n",
      "|30.0|-99.0 |30.01|-98.99            |0  |2838221505681162240|\n",
      "|30.0|-98.99|30.01|-98.97999999999999|1  |2838221471321423872|\n",
      "|30.0|-98.99|30.01|-98.97999999999999|1  |2838221522861031424|\n",
      "|30.0|-98.99|30.01|-98.97999999999999|1  |2838221505681162240|\n",
      "|30.0|-98.99|30.01|-98.97999999999999|1  |2838221557220769792|\n",
      "|30.0|-98.98|30.01|-98.97            |2  |2838221522861031424|\n",
      "|30.0|-98.98|30.01|-98.97            |2  |2838221540040900608|\n",
      "+----+------+-----+------------------+---+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"id\", monotonically_increasing_id()).withColumn(\"geohash\", explode(geoHashUDF(col(\"llat\"), col(\"llon\"), col(\"ulat\"), col(\"ulon\")))).show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"id\", monotonically_increasing_id()).withColumn(\"geohash\", explode(geoHashUDF(col(\"llat\"), col(\"llon\"), col(\"ulat\"), col(\"ulon\"))))\n",
    ".foreachPartition(partition => {\n",
    "    ConfigurationReader.setDeploymentID(\"db2eventstore-1583771517259\")\n",
    "    ConfigurationReader.setEventUser(\"rganti\")\n",
    "    ConfigurationReader.setEventPassword(\"password\")\n",
    "    ConfigurationReader.setConnectionEndpoints(\"172.30.176.26:18729;172.30.176.26:1100\")\n",
    "    val ctx1 = EventContext.getEventContext(\"EVENTDB\")\n",
    "    val tab1 = ctx1.getTable(tabName)\n",
    "    partition.grouped(500000).foreach(batch => {\n",
    "        val future: Future[InsertResult] = ctx1.batchInsertAsync(tab1, batch.toIndexedSeq)\n",
    "        val result: InsertResult = Await.result(future, Duration.Inf)\n",
    "        assert(result.successful)\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getExpandedBB: (geom: com.ibm.research.st.datamodel.geometry.ellipsoidal.IGeometryEG, distance: Double)com.ibm.research.st.datamodel.geometry.ellipsoidal.IBoundingBoxEG\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import com.ibm.research.st.STConstants\n",
    "import com.ibm.research.st.datamodel.geometry.ellipsoidal.{IBoundingBoxEG, IGeometryEG, IPointEG}\n",
    "import com.ibm.research.st.datamodel.geometry.ellipsoidal.impl.{BoundingBoxEG, PointEG}\n",
    "import com.ibm.research.st.util.{BitVector, LatLongUtil}\n",
    "\n",
    "\n",
    "def getExpandedBB(geom: IGeometryEG, distance: Double): IBoundingBoxEG = {\n",
    "  val bbox: IBoundingBoxEG = geom.getBoundingBox\n",
    "  val centroid: IPointEG = bbox.getCenter\n",
    "  val corners: java.util.List[IPointEG] = bbox.getCorners\n",
    "  val latShift: Double = Math.toDegrees(distance / STConstants.DEFAULT_DATUM.r0)\n",
    "  val lowerLat: Double = Math.max(STConstants.MIN_LATITUDE, corners.get(0).getLatitude - latShift)\n",
    "  val upperLat: Double = Math.min(STConstants.MAX_LATITUDE, corners.get(2).getLatitude + latShift)\n",
    "  val lonShift: Double = Math.toDegrees(distance / (STConstants.DEFAULT_DATUM.r0 * Math.cos(Math.toRadians(centroid.getLatitude))))\n",
    "  val lowerLon: Double = LatLongUtil.addLongitude(corners.get(0).getLongitude, -lonShift)\n",
    "  val upperLon: Double = LatLongUtil.addLongitude(corners.get(2).getLongitude, lonShift)\n",
    "  new BoundingBoxEG(new PointEG(lowerLat, lowerLon), new PointEG(upperLat, upperLon))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getLocationLookupKey: (geometry: com.ibm.research.st.datamodel.geometry.ellipsoidal.IGeometryEG, distance: Double, depth: Int)Array[com.ibm.research.st.util.BitVector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getLocationLookupKey(geometry: IGeometryEG, distance: Double, depth: Int): Array[BitVector] = {\n",
    "    val geohash = GeoHashEG.getInstance()\n",
    "    val bvs = geohash.tightNumberHashEncode(getExpandedBB(geometry, distance))\n",
    "    bvs.map(bv => {bv.truncate(depth); bv}).distinct\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getUint64KeyRange: (bv: com.ibm.research.st.util.BitVector, depth: Int)(Long, Long)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getUint64KeyRange(bv: BitVector, depth: Int) = {\n",
    "    val start = bv.getLongArray()(0)\n",
    "    val extraBitsNeeded = depth - bv.size\n",
    "    val end = start + (((1L << extraBitsNeeded) - 1L) << (java.lang.Long.SIZE - depth))\n",
    "    (start >>> 1, end >>> 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "es = org.apache.spark.sql.ibm.event.EventSession@7c4e39e1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.ibm.event.EventSession@7c4e39e1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val es = new EventSession(spark.sparkContext, \"EVENTDB\")\n",
    "es.openDatabase()\n",
    "es.setQueryReadOption(\"SnapshotNone\")\n",
    "es.loadEventTable(tabName).createOrReplaceTempView(tabName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM performance WHERE geohash between 3668699835189428224 and 3668699886729035776 or geohash between 3668699972628381696 and 3668700024167989248 or geohash between 3668699766469951488 and 3668699818009559040 or geohash between 3668699903908904960 and 3668699955448512512\n",
      "72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "condition = geohash between 3668699835189428224 and 3668699886729035776 or geohash between 3668699972628381696 and 3668700024167989248 or geohash between 3668699766469951488 and 3668699818009559040 or geohash between 3668699903908904960 and 3668699955448512512\n",
       "queryStr = SELECT * FROM performance WHERE geohash between 3668699835189428224 and 3668699886729035776 or geohash between 3668699972628381696 and 3668700024167989248 or geohash between 3668699766469951488 and 3668699818009559040 or geohash between 3668699903908904960 and 3668699955448512512\n",
       "results = [LLAT: double, LLON: double ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[LLAT: double, LLON: double ... 4 more fields]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val condition = getLocationLookupKey(new PointEG(43, -76), 1000, bitDepth)\n",
    ".map(bv => getUint64KeyRange(bv, bitDepth)).map(range => s\"geohash between ${range._1} and ${range._2}\").mkString(\" or \")\n",
    "val queryStr = s\"SELECT * FROM $tabName WHERE $condition\"\n",
    "println(queryStr)\n",
    "val results = es.sql(queryStr)\n",
    "println(results.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 722.646905508s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "r = scala.util.Random$@c317786d\n",
       "start = 308742324034850\n",
       "end = 309464970940358\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "309464970940358"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r = scala.util.Random\n",
    "val start = System.nanoTime()\n",
    "1 to 1000 foreach { _ =>\n",
    "    val lat = MIN_LAT + (MAX_LAT - MIN_LAT) * r.nextDouble\n",
    "    val lon = MIN_LON + (MAX_LON - MIN_LON) * r.nextDouble\n",
    "    val condition = getLocationLookupKey(new PointEG(lat, lon), 10000, bitDepth)\n",
    "    .map(bv => getUint64KeyRange(bv, bitDepth)).map(range => s\"geohash between ${range._1} and ${range._2}\").mkString(\" or \")\n",
    "    val queryStr = s\"SELECT * FROM $tabName WHERE $condition\"\n",
    "    es.sql(queryStr).count\n",
    "}\n",
    "val end = System.nanoTime()\n",
    "println(\"Total time: \" + (end - start) / 1e9 + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 690.243799377s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "r = scala.util.Random$@c317786d\n",
       "start = 307011145598258\n",
       "end = 307701389397635\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "307701389397635"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r = scala.util.Random\n",
    "val start = System.nanoTime()\n",
    "1 to 1000 foreach { _ =>\n",
    "    val lat = MIN_LAT + (MAX_LAT - MIN_LAT) * r.nextDouble\n",
    "    val lon = MIN_LON + (MAX_LON - MIN_LON) * r.nextDouble\n",
    "    val condition = getLocationLookupKey(new PointEG(lat, lon), 1000, bitDepth)\n",
    "    .map(bv => getUint64KeyRange(bv, bitDepth)).map(range => s\"geohash between ${range._1} and ${range._2}\").mkString(\" or \")\n",
    "    val queryStr = s\"SELECT * FROM $tabName WHERE $condition\"\n",
    "    es.sql(queryStr).count\n",
    "}\n",
    "val end = System.nanoTime()\n",
    "println(\"Total time: \" + (end - start) / 1e9 + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 497.043019402s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "r = scala.util.Random$@c317786d\n",
       "start = 307967706570956\n",
       "end = 308464749590358\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "308464749590358"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r = scala.util.Random\n",
    "val start = System.nanoTime()\n",
    "1 to 1000 foreach { _ =>\n",
    "    val lat = MIN_LAT + (MAX_LAT - MIN_LAT) * r.nextDouble\n",
    "    val lon = MIN_LON + (MAX_LON - MIN_LON) * r.nextDouble\n",
    "    val condition = getLocationLookupKey(new PointEG(lat, lon), 100, bitDepth)\n",
    "    .map(bv => getUint64KeyRange(bv, bitDepth)).map(range => s\"geohash between ${range._1} and ${range._2}\").mkString(\" or \")\n",
    "    val queryStr = s\"SELECT * FROM $tabName WHERE $condition\"\n",
    "    es.sql(queryStr).count\n",
    "}\n",
    "val end = System.nanoTime()\n",
    "println(\"Total time: \" + (end - start) / 1e9 + \"s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 with Spark",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}