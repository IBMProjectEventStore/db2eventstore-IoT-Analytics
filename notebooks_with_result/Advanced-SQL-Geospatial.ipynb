{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pre-requisite:\n",
    "Before running this notebook, you will have to:\n",
    "1. download the csv file named `dht_1k.csv`.\n",
    "stored under https://github.com/IBMProjectEventStore/db2eventstore-IoT-Analytics/tree/master/data.\n",
    "2. Go to the `Project tab` and load both above mentioned csv file into the current project as dataset.\n",
    "----\n",
    "**Note: This Notebook can only run in Python version >= 3.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from eventstore.oltp import EventContext\n",
    "from eventstore.sql import EventSession\n",
    "from eventstore.common import ConfigurationReader\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ConfigurationReader.setEventUser(\"user5\")\n",
    "ConfigurationReader.setEventPassword(\"EventStore20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#sparkSession = SparkSession.builder.config('spark.jars', './spark-time-series-sql.jar').appName(\"EventStore SQL in Python\").getOrCreate()\n",
    "sparkSession = SparkSession.builder.appName(\"EventStore SQL in Python\").getOrCreate()\n",
    "eventSession = EventSession(sparkSession.sparkContext, \"EVENTDB\")\n",
    "eventSession.set_query_read_option(\"SnapshotNow\")\n",
    "eventSession.open_database()\n",
    "ctx = EventContext.get_event_context(\"EVENTDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eventSession._jvm.org.apache.spark.sql.types.SqlGeometry.registerAll(eventSession._jsparkSession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from eventstore.catalog import TableSchema\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "table_names = ctx.get_names_of_tables()\n",
    "for idx, name in enumerate(table_names):\n",
    "    print(idx, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def datetime_converter(datetime_string):\n",
    "    # (1) Convert to datetime format\n",
    "    utc_time = datetime.strptime(datetime_string.split('.000Z')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    return int((utc_time - datetime(1970, 1, 1)).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define table schema to be created\n",
    "with EventContext.get_event_context(\"EVENTDB\") as ctx:\n",
    "    schema = StructType([\n",
    "        StructField(\"sensor_id\", IntegerType(), nullable = False),\n",
    "        StructField(\"timestamp\", IntegerType(), nullable = False),\n",
    "        StructField(\"location\", IntegerType(), nullable = False),\n",
    "        StructField(\"humidity\", FloatType(), nullable = True),\n",
    "        StructField(\"temperature\", FloatType(), nullable = False),\n",
    "        StructField(\"LAT\", FloatType(), nullable = False),\n",
    "        StructField(\"LON\", FloatType(), nullable = False),\n",
    "        StructField(\"sensor_type\", StringType(), nullable = False)\n",
    "    ])  \n",
    "    table_schema = TableSchema(\"dht_full_table\", schema,\n",
    "                                sharding_columns=[\"sensor_id\"],\n",
    "                                pk_columns=[\"timestamp\",\"sensor_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## create table and loading data for DHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER5.DHT_FULL_TABLE\n"
     ]
    }
   ],
   "source": [
    "# try create table if not exist\n",
    "# try:\n",
    "#     ctx.drop_table(\"dht_full_table\")\n",
    "# except Exception as error:\n",
    "#     print(error)\n",
    "try:\n",
    "    ctx.create_table(table_schema)\n",
    "except Exception as error:\n",
    "    print(error)\n",
    "    \n",
    "table_names = ctx.get_names_of_tables()\n",
    "for idx, name in enumerate(table_names):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dht_table = eventSession.load_event_table(\"dht_full_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResolvedTableSchema(tableName=USER5DHT_FULL_TABLE, schema=StructType(List(StructField(SENSOR_ID,IntegerType,false),StructField(TIMESTAMP,IntegerType,false),StructField(LOCATION,IntegerType,false),StructField(HUMIDITY,FloatType,true),StructField(TEMPERATURE,FloatType,false),StructField(LAT,FloatType,false),StructField(LON,FloatType,false),StructField(SENSOR_TYPE,StringType,false))), sharding_columns=['SENSOR_ID'], pk_columns=['TIMESTAMP', 'SENSOR_ID'], partition_columns=None, schema_name=Some(USER5))\n"
     ]
    }
   ],
   "source": [
    "# ingest data into table\n",
    "import os\n",
    "resolved_table_schema = ctx.get_table(\"dht_full_table\")\n",
    "print(resolved_table_schema)\n",
    "with open(os.environ['DSX_PROJECT_DIR']+'/datasets/dht_1k.csv') as f:\n",
    "    f.readline()\n",
    "    content = f.readlines()\n",
    "content = [l.split(\",\") for l in content]\n",
    "batch = [dict(sensor_id=int(c[5]), timestamp=datetime_converter(c[7]), location=int(c[0]), \\\n",
    "              humidity=float(c[2]),temperature=float(c[1]),lat=float(c[3]),lon=float(c[4]),sensor_type=str(c[6])) for c in content]\n",
    "ctx.batch_insert(resolved_table_schema, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41569"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify ingested result\n",
    "dht_table = eventSession.load_event_table(\"dht_full_table\")\n",
    "dht_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dht_table.createOrReplaceTempView(\"dht_full_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   41569|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eventSession.sql(\"select count(*) from dht_full_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Objective: Group Sensors into Geohashes by using SQL with ST Support.\n",
    "### Use SQL\n",
    "We utilize SQL for this step because the volume of raw data can be huge - there are a lot of sensors and each sensor has a lot of readings per day. We use sql so that we can avoid pulling the whole raw data which could cause some serious memory issues. It is often suggested to, whenever possible, run Spatial operations in SQL first as a preprocessing step to reduce the complexity and volume of the data.\n",
    "### SQL with ST Support\n",
    "The key part in this query is the ST support - \n",
    "- `ST_Point(lon, lat)` creates a spatial ST_Point object from given latitude and longitude in the raw data.\n",
    "- `ST_ContainingGeohash(ST_Point, distance_buffer)` encode the point into its geohash.\n",
    "\n",
    "Everything else is just the normal SQL query - \n",
    "- We get geohash, humidity from the raw dataset.\n",
    "- We group these readings by geohash and calcuate average reading for each geohash.\n",
    "\n",
    "For a full list of geospatial functions available on Cloud SQL Query, [click here](https://www.ibm.com/support/knowledgecenter/SS6NHC/com.ibm.swg.im.dashdb.analytics.doc/doc/geo_functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stmt = \"\"\"\n",
    "    SELECT geohash, AVG(humidity) as avg_h\n",
    "    FROM(\n",
    "        SELECT cast(ST_ContainingGeohash(ST_Point(lon, lat), 300) as string) as geohash, humidity\n",
    "        FROM dht_full_table\n",
    "    )\n",
    "    GROUP BY geohash\n",
    "\"\"\"\n",
    "\n",
    "eventSession.sql(stmt).createOrReplaceTempView(\"dht_spatial_agg_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Objectives - \n",
    "\n",
    "- Since all the humidity sensors are discretely and spatially distributed, we want to group them into areas based on their locations so that we are able to tell the humidity for different areas instead of discrete points. To achieve this, we will utilize Geohashes since each geohash represents a grid area on the earth, and we compute the geohash for each sensor location and group them by geohashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|             geohash|             avg_h|\n",
      "+--------------------+------------------+\n",
      "|11010000011000001...| 77.29051867398348|\n",
      "|11010000100100000...| 64.97717030761326|\n",
      "|11010000001110011...|58.117268643045534|\n",
      "|11010000001110011...| 77.50434785925823|\n",
      "|11010000001111100...| 66.54934394039975|\n",
      "|11010000001110011...| 87.21721242391146|\n",
      "|11010000100100011100| 59.80641026263471|\n",
      "|11010000001110011...| 63.08857686510693|\n",
      "|11010000010010110...| 63.79313880487195|\n",
      "|11010000001110011...|12.257560955722157|\n",
      "|11010000001101000...| 68.66730075434118|\n",
      "|11010001101100001...| 81.45821407398726|\n",
      "|11010000110001101...| 72.19365398246984|\n",
      "|11010000001110011001| 47.90347020425701|\n",
      "|11000101110101010...|51.095640883634985|\n",
      "|11010000110001101...| 97.05609872419103|\n",
      "|11010000001110010...|57.990811994612386|\n",
      "|11010000011000001...| 51.01045743406208|\n",
      "|11010000001111010...| 78.36612912916368|\n",
      "| 1101000000111001001| 51.00362491833121|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eventSession.sql(\"select * from dht_spatial_agg_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#decode the geohashes and then find the top humidity location\n",
    "decode_stmt = \"\"\"\n",
    "    SELECT geohash, ST_X(ST_BoundingBoxCenter(ST_Envelope(ST_GeohashDecode(geohash)))) as lon, \n",
    "    ST_Y(ST_BoundingBoxCenter(ST_Envelope(ST_GeohashDecode(geohash)))) AS lat, avg_h\n",
    "    FROM dht_spatial_agg_table\n",
    "    ORDER BY avg_h desc\n",
    "\"\"\"\n",
    "\n",
    "eventSession.sql(decode_stmt).createOrReplaceTempView(\"dht_spatial_decoded_agg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = eventSession.sql(\"select * from dht_spatial_decoded_agg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+----------------+------------------+\n",
      "|             geohash|           lon|             lat|             avg_h|\n",
      "+--------------------+--------------+----------------+------------------+\n",
      "|11010000110001101...|13.46923828125| 52.547607421875| 97.05609872419103|\n",
      "| 1101000001101111011|   10.37109375|     53.26171875| 88.24107743750439|\n",
      "|11010000001100100...| 7.71240234375|  48.01025390625| 87.74399086785695|\n",
      "|11010000001110011...|   9.228515625|   48.8232421875| 87.21721242391146|\n",
      "|11010001101100001...| 18.2373046875|   59.2822265625| 81.45821407398726|\n",
      "|11010000001111010...|9.063720703125| 50.086669921875| 78.36612912916368|\n",
      "|11010000001110011...| 9.25048828125|  48.75732421875| 77.50434785925823|\n",
      "|11010000011000001...| 6.74560546875| 50.855712890625| 77.29051867398348|\n",
      "|11010000110001101...| 13.4912109375|   52.5146484375| 72.19365398246984|\n",
      "|11010000100100000...|11.53564453125| 48.153076171875| 70.01383566990478|\n",
      "|11010000001101000...|   6.064453125|    49.482421875| 68.66730075434118|\n",
      "|11010000001111100...|9.986572265625| 49.801025390625| 66.54934394039975|\n",
      "|11010000100100000...|11.62353515625|  48.18603515625| 64.97717030761326|\n",
      "|11010000001110011...| 9.25048828125| 48.768310546875| 64.34081949341098|\n",
      "|11010000010010101...| 5.16357421875|  51.17431640625| 63.79920448173176|\n",
      "|11010000010010110...|4.888916015625|51.6961669921875| 63.79313880487195|\n",
      "|11010000001110011...| 9.16259765625|  48.75732421875| 63.08857686510693|\n",
      "|11010000011000000...| 6.13037109375|  50.82275390625|60.310458692498166|\n",
      "|11010000100100011100|   12.12890625|    48.955078125| 59.80641026263471|\n",
      "|11010000001110011...| 9.16259765625| 48.768310546875|58.117268643045534|\n",
      "+--------------------+--------------+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7",
   "language": "python",
   "name": "py2localspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
