{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pre-requisite:\n",
    "Before running this notebook, you will have to:\n",
    "1. download the csv file named `dht_1k.csv` \n",
    "stored under https://github.com/IBMProjectEventStore/db2eventstore-IoT-Analytics/tree/master/data.\n",
    "2. Go to the `Project tab` and load both above mentioned csv files into the current project as dataset.\n",
    "----\n",
    "**Note: This Notebook can only run in Python version >= 3.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from eventstore.oltp import EventContext\n",
    "from eventstore.sql import EventSession\n",
    "from eventstore.common import ConfigurationReader\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ConfigurationReader.setEventUser(\"\")\n",
    "ConfigurationReader.setEventPassword(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#sparkSession = SparkSession.builder.config('spark.jars', './spark-time-series-sql.jar').appName(\"EventStore SQL in Python\").getOrCreate()\n",
    "sparkSession = SparkSession.builder.appName(\"EventStore SQL in Python\").getOrCreate()\n",
    "eventSession = EventSession(sparkSession.sparkContext, \"EVENTDB\")\n",
    "eventSession.set_query_read_option(\"SnapshotNow\")\n",
    "eventSession.open_database()\n",
    "ctx = EventContext.get_event_context(\"EVENTDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eventSession._jvm.org.apache.spark.sql.types.SqlGeometry.registerAll(eventSession._jsparkSession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from eventstore.catalog import TableSchema\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ADMIN.DHT_TABLE\n",
      "1 ADMIN.SDS_TABLE\n",
      "2 ADMIN.IOTPERF\n",
      "3 ADMIN.DHT_FULL_TABLE\n"
     ]
    }
   ],
   "source": [
    "table_names = ctx.get_names_of_tables()\n",
    "for idx, name in enumerate(table_names):\n",
    "    print(idx, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def datetime_converter(datetime_string):\n",
    "    # (1) Convert to datetime format\n",
    "    utc_time = datetime.strptime(datetime_string.split('.000Z')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    return int((utc_time - datetime(1970, 1, 1)).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showPartitionInfo(df):\n",
    "    # show partition number and number of records in a partition in the given Spark dataframe\n",
    "    #@df: Spark DataFrame\n",
    "    print(\"- number of partitions prior to time series (after loading table): \",df.rdd.getNumPartitions())\n",
    "    print(\"- partition sizes prior to time series (after loading table): \", df.rdd.mapPartitions(lambda s: iter([sum(1 for _ in s)])).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Create table, ingest data table, load table as Spark Dataframe and repartition the dataframe properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define table schema to be created\n",
    "with EventContext.get_event_context(\"EVENTDB\") as ctx:\n",
    "    schema = StructType([\n",
    "        StructField(\"sensor_id\", IntegerType(), nullable = False),\n",
    "        StructField(\"timestamp\", IntegerType(), nullable = False),\n",
    "        StructField(\"location\", IntegerType(), nullable = False),\n",
    "        StructField(\"humidity\", FloatType(), nullable = True),\n",
    "        StructField(\"temperature\", FloatType(), nullable = False),\n",
    "        StructField(\"LAT\", FloatType(), nullable = False),\n",
    "        StructField(\"LON\", FloatType(), nullable = False),\n",
    "        StructField(\"sensor_type\", StringType(), nullable = False)\n",
    "    ])  \n",
    "    table_schema = TableSchema(\"dht_full_table\", schema,\n",
    "                                sharding_columns=[\"sensor_id\"],\n",
    "                                pk_columns=[\"timestamp\",\"sensor_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## create table and loading data for DHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not created. Table may already exist.\n",
      "An error occurred while calling o2.createTable.\n",
      ": com.ibm.event.EventException: createTableWithIndex() : createTable() : client request failed: DB2 SQL Error: SQLCODE=-601, SQLSTATE=42710, SQLERRMC=ADMIN.DHT_FULL_TABLE;TABLE, DRIVER=4.25.4\n",
      "\tat com.ibm.event.oltp.EventContext.createTableWithIndexInternal(EventContext.scala:1144)\n",
      "\tat com.ibm.event.oltp.EventContext.createTable(EventContext.scala:993)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "ADMIN.DHT_TABLE\n",
      "ADMIN.SDS_TABLE\n",
      "ADMIN.IOTPERF\n",
      "ADMIN.DHT_FULL_TABLE\n"
     ]
    }
   ],
   "source": [
    "# try create table if not exist\n",
    "# try:\n",
    "#     ctx.drop_table(\"dht_full_table\")\n",
    "# except Exception as error:\n",
    "#     print(error)\n",
    "try:\n",
    "    ctx.create_table(table_schema)\n",
    "except Exception as error:\n",
    "    print(\"Table not created. Table may already exist.\")\n",
    "    print(error)\n",
    "    \n",
    "table_names = ctx.get_names_of_tables()\n",
    "for idx, name in enumerate(table_names):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dht_table = eventSession.load_event_table(\"dht_full_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResolvedTableSchema(tableName=ADMINDHT_FULL_TABLE, schema=StructType(List(StructField(SENSOR_ID,IntegerType,false),StructField(TIMESTAMP,IntegerType,false),StructField(LOCATION,IntegerType,false),StructField(HUMIDITY,FloatType,true),StructField(TEMPERATURE,FloatType,false),StructField(LAT,FloatType,false),StructField(LON,FloatType,false),StructField(SENSOR_TYPE,StringType,false))), sharding_columns=['SENSOR_ID'], pk_columns=['TIMESTAMP', 'SENSOR_ID'], partition_columns=None, schema_name=Some(ADMIN))\n"
     ]
    }
   ],
   "source": [
    "# ingest data into table\n",
    "import os\n",
    "resolved_table_schema = ctx.get_table(\"dht_full_table\")\n",
    "print(resolved_table_schema)\n",
    "with open(os.environ['DSX_PROJECT_DIR']+'/datasets/dht_1k.csv') as f:\n",
    "    f.readline()\n",
    "    content = f.readlines()\n",
    "content = [l.split(\",\") for l in content]\n",
    "batch = [dict(sensor_id=int(c[5]), timestamp=datetime_converter(c[7]), location=int(c[0]), \\\n",
    "              humidity=float(c[2]),temperature=float(c[1]),lat=float(c[3]),lon=float(c[4]),sensor_type=str(c[6])) for c in content]\n",
    "ctx.batch_insert(resolved_table_schema, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41569"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify ingested result\n",
    "dht_table = eventSession.load_event_table(\"dht_full_table\")\n",
    "dht_table.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartitioning\n",
    "Optimize parallelism by repartitioning Note that when the query is pushed down to Db2 Event Store and the data is retrieved, the data will be received by Spark as one single partitioned data frame. It's necessary for the user to explicitly repartition the dataframe. It's suggested that one partition is created for each CPU core in the Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- number of partitions prior to time series (after loading table):  1\n",
      "- partition sizes prior to time series (after loading table):  [41569]\n"
     ]
    }
   ],
   "source": [
    "showPartitionInfo(dht_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- number of partitions prior to time series (after loading table):  48\n",
      "- partition sizes prior to time series (after loading table):  [866, 867, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866, 866]\n"
     ]
    }
   ],
   "source": [
    "dht_table = dht_table.repartition(48)\n",
    "showPartitionInfo(dht_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dht_table.createOrReplaceTempView(\"dht_full_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+--------+-----------+------+-----+-----------+\n",
      "|SENSOR_ID| TIMESTAMP|LOCATION|HUMIDITY|TEMPERATURE|   LAT|  LON|SENSOR_TYPE|\n",
      "+---------+----------+--------+--------+-----------+------+-----+-----------+\n",
      "|     1309|1504144549|     647|    99.9|       20.0|48.839|9.315|      DHT22|\n",
      "|     1309|1504151592|     647|    99.9|       19.4|48.839|9.315|      DHT22|\n",
      "|     1309|1504158635|     647|    99.9|       20.0|48.839|9.315|      DHT22|\n",
      "|     1309|1504168937|     647|    99.9|       22.6|48.839|9.315|      DHT22|\n",
      "|     1309|1504185747|     647|    90.1|       19.4|48.839|9.315|      DHT22|\n",
      "+---------+----------+--------+--------+-----------+------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eventSession.sql(\"select * from dht_full_table LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Objective: Group Sensors into Geohashes by using SQL with ST Support.\n",
    "### Use SQL\n",
    "We utilize SQL for this step because the volume of raw data can be huge - there are a lot of sensors and each sensor has a lot of readings per day. We use sql so that we can avoid pulling the whole raw data which could cause some serious memory issues. It is often suggested to, whenever possible, run Spatial operations in SQL first as a preprocessing step to reduce the complexity and volume of the data.\n",
    "### SQL with ST Support\n",
    "The key part in this query is the ST support - \n",
    "- `ST_Point(lon, lat)` creates a spatial ST_Point object from given latitude and longitude in the raw data.\n",
    "- `ST_ContainingGeohash(ST_Point, distance_buffer)` encode the point into its geohash.\n",
    "\n",
    "Everything else is just the normal SQL query - \n",
    "- We get geohash, humidity from the raw dataset.\n",
    "- We group these readings by geohash and calcuate average reading for each geohash.\n",
    "\n",
    "For a full list of geospatial functions available on SQL Query, [click here](https://www.ibm.com/support/knowledgecenter/SSGNPV_2.0.0/com.ibm.swg.im.dashdb.analytics.doc/doc/geo_intro.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stmt = \"\"\"\n",
    "    SELECT geohash, AVG(humidity) as avg_h\n",
    "    FROM(\n",
    "        SELECT cast(ST_ContainingGeohash(ST_Point(lon, lat), 300) as string) as geohash, humidity\n",
    "        FROM dht_full_table\n",
    "    )\n",
    "    GROUP BY geohash\n",
    "\"\"\"\n",
    "\n",
    "eventSession.sql(stmt).createOrReplaceTempView(\"dht_spatial_agg_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Objectives - \n",
    "\n",
    "- Since all the humidity sensors are discretely and spatially distributed, we want to group them into areas based on their locations so that we are able to tell the humidity for different areas instead of discrete points. To achieve this, we will utilize Geohashes since each geohash represents a grid area on the earth, and we compute the geohash for each sensor location and group them by geohashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|             geohash|             avg_h|\n",
      "+--------------------+------------------+\n",
      "|11010000001110011...|58.117268643045534|\n",
      "|11010000011000001...| 77.29051867398348|\n",
      "|11010000100100000...| 64.97717030761326|\n",
      "|11010000001110011...| 77.50434785925823|\n",
      "|11010000001111100...| 66.54934394039975|\n",
      "|11010000001110011...| 87.21721242391146|\n",
      "|11010000100100011100| 59.80641026263471|\n",
      "|11010000001110011...| 63.08857686510693|\n",
      "|11010000010010110...| 63.79313880487195|\n",
      "|11010000001110011...|12.257560955722157|\n",
      "|11010000001101000...| 68.66730075434118|\n",
      "|11010001101100001...| 81.45821407398726|\n",
      "|11010000110001101...| 72.19365398246984|\n",
      "|11010000001110011001| 47.90347020425701|\n",
      "|11000101110101010...|51.095640883634985|\n",
      "|11010000110001101...| 97.05609872419103|\n",
      "|11010000001110010...|57.990811994612386|\n",
      "|11010000011000001...| 51.01045743406208|\n",
      "|11010000001111010...| 78.36612912916368|\n",
      "| 1101000000111001001| 51.00362491833121|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eventSession.sql(\"select * from dht_spatial_agg_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#decode the geohashes and then find the top humidity location\n",
    "decode_stmt = \"\"\"\n",
    "    SELECT geohash, ST_X(ST_BoundingBoxCenter(ST_Envelope(ST_GeohashDecode(geohash)))) as lon, \n",
    "    ST_Y(ST_BoundingBoxCenter(ST_Envelope(ST_GeohashDecode(geohash)))) AS lat, avg_h\n",
    "    FROM dht_spatial_agg_table\n",
    "    ORDER BY avg_h desc\n",
    "\"\"\"\n",
    "\n",
    "eventSession.sql(decode_stmt).createOrReplaceTempView(\"dht_spatial_decoded_agg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = eventSession.sql(\"select * from dht_spatial_decoded_agg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+----------------+------------------+\n",
      "|             geohash|           lon|             lat|             avg_h|\n",
      "+--------------------+--------------+----------------+------------------+\n",
      "|11010000110001101...|13.46923828125| 52.547607421875| 97.05609872419103|\n",
      "| 1101000001101111011|   10.37109375|     53.26171875| 88.24107743750439|\n",
      "|11010000001100100...| 7.71240234375|  48.01025390625| 87.74399086785695|\n",
      "|11010000001110011...|   9.228515625|   48.8232421875| 87.21721242391146|\n",
      "|11010001101100001...| 18.2373046875|   59.2822265625| 81.45821407398726|\n",
      "|11010000001111010...|9.063720703125| 50.086669921875| 78.36612912916368|\n",
      "|11010000001110011...| 9.25048828125|  48.75732421875| 77.50434785925823|\n",
      "|11010000011000001...| 6.74560546875| 50.855712890625| 77.29051867398348|\n",
      "|11010000110001101...| 13.4912109375|   52.5146484375| 72.19365398246984|\n",
      "|11010000100100000...|11.53564453125| 48.153076171875| 70.01383566990478|\n",
      "|11010000001101000...|   6.064453125|    49.482421875| 68.66730075434118|\n",
      "|11010000001111100...|9.986572265625| 49.801025390625| 66.54934394039975|\n",
      "|11010000100100000...|11.62353515625|  48.18603515625| 64.97717030761326|\n",
      "|11010000001110011...| 9.25048828125| 48.768310546875| 64.34081949341098|\n",
      "|11010000010010101...| 5.16357421875|  51.17431640625| 63.79920448173176|\n",
      "|11010000010010110...|4.888916015625|51.6961669921875| 63.79313880487195|\n",
      "|11010000001110011...| 9.16259765625|  48.75732421875| 63.08857686510693|\n",
      "|11010000011000000...| 6.13037109375|  50.82275390625|60.310458692498166|\n",
      "|11010000100100011100|   12.12890625|    48.955078125| 59.80641026263471|\n",
      "|11010000001110011...| 9.16259765625| 48.768310546875|58.117268643045534|\n",
      "+--------------------+--------------+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with Watson Studio Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
