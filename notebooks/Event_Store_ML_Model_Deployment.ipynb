{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# IBM Db2 Event Store - Machine Learning Modeling and Model Deployment \n",
    "IBM Db2 Event Store is a hybrid transactional/analytical processing (HTAP) system. This notebook illustrates the machine learning modeling and model deployment using IBM Db2 Event Store.\n",
    "\n",
    "***Pre-Req: Event Store Data Analytics***\n",
    "\n",
    "When finish this demo, you will learn:\n",
    "- How to build a machine learning model\n",
    "- How to save and deploy the model\n",
    "- How to make realtime predictions with the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setting basic import clauses used by this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from eventstore.oltp import EventContext\n",
    "from eventstore.sql import EventSession\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setting the IP address to connect to your IBM Db2 Event Store cluster\n",
    "\n",
    "For this, you will need to find out the connection string to your IBM Db2 Event Store cluster.\n",
    "\n",
    "Perform the following steps:\n",
    "\n",
    "- Replace the IP address in the below program code with the IP address of your local host\n",
    "- Then execute the program cell below. It will connect to the IBM Db2 Event Store cluster in the provided connection string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from eventstore.common import ConfigurationReader\n",
    "\n",
    "ip = \"9.30. < TO REPLACE >\"\n",
    "\n",
    "endpoint = ip + ':1101'\n",
    "\n",
    "print(\"Endpoint: \"+ endpoint)\n",
    "\n",
    "ConfigurationReader.setConnectionEndpoints(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Opening a database\n",
    "The following code is used to open a database to be able to access its tables and data.\n",
    "\n",
    "Run the command in the next program cell to define the database name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dbName = \"TESTDB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To run Spark SQL queries, you must set up a Db2 Event Store Spark session. The EventSession class extends the optimizer of the SparkSession class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"EventStore SQL in Python\").getOrCreate()\n",
    "eventSession = EventSession(sparkSession.sparkContext, dbName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now you can execute the command to open the database in the event session you created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eventSession.open_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Access an existing table in the database\n",
    "The following code section retrieves the names of all tables that exist in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with EventContext.get_event_context(dbName) as ctx:\n",
    "   print(\"Event context successfully retrieved.\")\n",
    "\n",
    "table_names = ctx.get_names_of_tables()\n",
    "for idx, name in enumerate(table_names):\n",
    "   print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have the name of the existing table. We then load the corresponding table and get the data frame references to access the table with query. The following code loads the tables and creates a temp view with the same name as the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tabName = \"IOT_TEMP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tab = eventSession.load_event_table(tabName)\n",
    "tab.createOrReplaceTempView(tabName)\n",
    "print(\"Table \"+tabName+\" successfully loaded and temp view created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The next code retrieves the schema of the table we want to investigate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    resolved_table_schema = ctx.get_table(tabName)\n",
    "    print(resolved_table_schema)\n",
    "except Exception as err:\n",
    "    print(\"Table not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Machine Learning Modeling\n",
    "This section shows how to build a machine learning model with the data stored in the IBM Db2 Event Store database.\n",
    "\n",
    "### Recall from the *Event_Store_Data_Analytics* notebook\n",
    "- There are two input variables: ambient temperature and power consumption. The dependent variable is the sensor temperature reading.\n",
    "- All features follow normal distribution.\n",
    "- There is an obvious linear relationship between each independent variable and the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's try generating a linear model to predict sensor temperature with power consumption and ambient temperature with the data stored in the IBM Db2 Event Store database table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First import the relevant pyspark machine learning libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following cell builds a new spark SQL dataframe from the `tab` dataframe, and prints out the `variable_df` dataframe schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variables = [\"ambient_temp\", \"power\"]\n",
    "variable_df = tab.select(col(\"temperature\").alias(\"label\"), *variables)\n",
    "variable_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we split the dataframe into training set and test set at a percentage of 0.75 and 0.25.\n",
    "\n",
    "We first build and train the model on the training set, then evaluate the model performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training, test = variable_df.randomSplit([0.75, 0.25], 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model is built as a pipeline. There are three stages in the model pipeline, namely, *vector assembly*, *standarization*, and *model definition*. \n",
    "\n",
    "In the following cell we execute the three stages.\n",
    "\n",
    "The training set is first assembled in to a dense dense vector. Then, the dense vector is standarized to a standard normal distribution. Finally the linear model is defined with regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=variables, outputCol=\"unscaled_variables\")\n",
    "standardScaler = StandardScaler(inputCol=\"unscaled_variables\", outputCol=\"features\")\n",
    "linear_model = LinearRegression(maxIter=10, regParam=.01)\n",
    "\n",
    "stages = [vectorAssembler, standardScaler, linear_model]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model is then trained on the training set. The trained model is used to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(training)\n",
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the following cell we just show the first 10 rows out of the approximately 250 thousand in the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prediction.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Evaluation\n",
    "The performance of the linear model we just built can be evaluated using multiple error metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We first load and define a regression evaluator using pyspark library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We then evaluate the model performance with multiple error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(prediction)\n",
    "\n",
    "mae = evaluator.evaluate(prediction, {evaluator.metricName: \"mae\"})\n",
    "\n",
    "r2 = evaluator.evaluate(prediction, {evaluator.metricName: \"r2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally we put the error metrics into a dataframe to help visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "error_df = {\"r2\":r2, \"mae\":mae, \"rmse\":rmse}\n",
    "error_df = pd.DataFrame.from_dict(error_df, orient=\"index\")\n",
    "error_df.columns = [\"error metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show error metrics\n",
    "error_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Model Summarization**  \n",
    "The r2 metrics shows the percentage of the variance in the data that is explained by the model. Our model has a high r2 value that is very close to 1, meaning most of the variance in the test data can be explained with our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model Deployment\n",
    "After the model is trained in the notebook, a user can save the model into Watson Studio Local by using the save function in the `dsx_ml` library. Then the model can be used to generate real time online scoring on the data streamed into IBM Db2 Event Store.\n",
    "\n",
    "Users of IBM Watson Studio are allowed to save the spark, keras and scikit-learn models trained on the data stored in the IBM Db2 Event Store.\n",
    "\n",
    "For more information, please visit [IBM Watson Studio Locao - Save Model](https://content-dsxlocal.mybluemix.net/docs/content/SSAS34_current/local-dev/ml-notebook-doc.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first step is to save the model with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from dsx_ml.ml import save\n",
    "model_name = \"Event_Store_IOT_Sensor_Temperature_Prediction_Model\"\n",
    "saved_model = save(name=model_name, \n",
    "                   model=model,\n",
    "                   test_data=test,\n",
    "                   algorithm_type=\"Regression\",\n",
    "                   source='Event_Store_Modeling.ipynb',\n",
    "                   description=\"Linear regression model to predict IOT sensor temperature\"\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With the saved model we then define a header that contains authorization, which will be sent to the endpoint, and then retrieve the endpoint to the saved model to allow us to externally access it. Note that the host name `dsxl-api` needs to be replaced with the corresponding external IP address of your IBM Watson Studio cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "header_online = {'Content-Type': 'application/json', 'Authorization': os.environ['DSX_TOKEN']}\n",
    "# Retrive the endpoint to the saved model\n",
    "print(saved_model[\"scoring_endpoint\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Make a Prediction with the Deployed Model\n",
    "Now the model has been saved and deployed. After deployment, the endpoint of model can be used to make a prediction for new data using the online scoring service.  \n",
    "\n",
    "The following sample code snippet calls the scoring endpoint to make predictions on the new data. The prediction can be made on single datum, or on batch data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First create a sample datum to be predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_data = {\"deviceID\" : 2, \"sensorID\": 24, \"ts\": 1541430459386, \"ambient_temp\": 30, \"power\": 10}\n",
    "new_data2 = {\"deviceID\" : 1, \"sensorID\": 12, \"ts\": 1541230400000, \"ambient_temp\": 16, \"power\": 50}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Single datum prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "payload_scoring = [new_data]\n",
    "scoring_response = requests.post(saved_model[\"scoring_endpoint\"], json=payload_scoring, headers=header_online, verify=False)\n",
    "\n",
    "print(scoring_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Because this is a regression model, we can retrieve the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scoring_response.json()[\"object\"][\"output\"][\"predictions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "payload_scoring = [new_data, new_data2]\n",
    "scoring_response = requests.post(saved_model[\"scoring_endpoint\"], json=payload_scoring, headers=header_online, verify=False)\n",
    "\n",
    "print(scoring_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scoring_response.json()[\"object\"][\"output\"][\"predictions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Scoring with Curl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the machine learning model is deployed. Online scoreing can also be performed in commandline with curl.  \n",
    "\n",
    "```bash\n",
    "To use curl for online scoring, user has to first obtain a authentication token from the cluster by running following command from the command line.\n",
    "\n",
    "A json file will be returned with an \"accessToken\" that can be used\n",
    "Example command is:\n",
    "\n",
    "    curl -k -X GET \"https://<HostIP>/v1/preauth/validateAuth\" -u admin:password\n",
    "\n",
    "With the authenticati on token, user can run the following commands from the command line for single or batch scoring:\n",
    "\n",
    "#single scoring\n",
    "    echo \"Single Scoring\" \n",
    "    curl -k -X POST \"https://$CLUSTER_IP/v3/project/score/Python27/spark-2.0/IOT_demo/Event_Store_IOT_Sensor_Temperature_Prediction_Model/5\"  -H 'Content-Type: application/json' -H \"Authorization: Bearer $accessToken\" -d ' { \"deviceID\" : 2, \"sensorID\": 24, \"ts\": 1541430459386, \"ambient_temp\": 30, \"power\": 10 }'\n",
    "\n",
    "#batch scoring\n",
    "    echo \"Batch Scoring\" \n",
    "    curl -k -X POST \"https://$CLUSTER_IP/v3/project/score/Python27/spark-2.0/IOT_demo/Event_Store_IOT_Sensor_Temperature_Prediction_Model/5\"  -H 'Content-Type: application/json' -H \"Authorization: Bearer $accessToken\" -d '[ { \"deviceID\" : 2, \"sensorID\": 24, \"ts\": 1541430459386, \"ambient_temp\": 30, \"power\": 10 }, {\"deviceID\" : 1, \"sensorID\": 12, \"ts\": 1541230400000, \"ambient_temp\": 16, \"power\": 50}]'\n",
    "\n",
    "```\n",
    "Please find the bash scripts demonstrating online scoring with curl in the `/curl` directory of this git repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Summary\n",
    "This demo introduced you to the IBM Db2 Event Store API for Machine Learning and Model Deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with Watson Studio Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
