{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Querying on IBM Db2 Event Store Database Table\n",
    "IBM Db2 Event Store is a hybrid transactional/analytical processing (HTAP) system. This notebook will demonstrate the best practices querying the table stored in IBM Db2 Event Store database.\n",
    "\n",
    "In the previous session we have already created a database, a table with an index, and seen how to ingest sample data into the table using the \"EventStore_Table_Creation\" notebook.\n",
    "\n",
    "***Pre-Req: EventStore_Table_Creation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Open the database\n",
    "\n",
    "The cells in this section are used to open the database and create a temp view for the table that we created previously.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run the commands in the following cells to define access to the existing database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from eventstore.oltp import EventContext\n",
    "from eventstore.sql import EventSession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Setting the IP address to connect to your IBM Db2 Event Store cluster\n",
    "\n",
    "For this, you will need to find out the connection string to your IBM Db2 Event Store cluster.\n",
    "\n",
    "Perform the following steps:\n",
    "\n",
    "- Replace the IP address in the below program code with the IP address of your local host\n",
    "- Then execute the program cell below. It will connect to the IBM Db2 Event Store cluster in the provided connection string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from eventstore.common import ConfigurationReader\n",
    "\n",
    "ip = \"9.30. <TO BE COMPLETED > \"\n",
    "\n",
    "endpoint = ip + ':1101'\n",
    "\n",
    "print(\"Endpoint: \"+ endpoint)\n",
    "\n",
    "ConfigurationReader.setConnectionEndpoints(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dbName = \"TESTDB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To run Spark SQL queries, you first have to set up a Db2 Event Store Spark session. The EventSession class extends the optimizer of the SparkSession class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"EventStore SQL in Python\").getOrCreate()\n",
    "eventSession = EventSession(sparkSession.sparkContext, dbName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The next cell opens the database to allow operations against it to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eventSession.open_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With the following cells we can list all existing tables and then load the table we previously created into the tab DataFrame reference. Note that we are defining the `tab` DataFrame reference that will be used later on in this notebook to create a temp view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with EventContext.get_event_context(dbName) as ctx:\n",
    "   print(\"Event context successfully retrieved.\")\n",
    "\n",
    "table_names = ctx.get_names_of_tables()\n",
    "for idx, name in enumerate(table_names):\n",
    "   print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tabName = \"IOT_TEMP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tab = eventSession.load_event_table(tabName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's recall the table schema we previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    resolved_table_schema = ctx.get_table(tabName)\n",
    "    print(resolved_table_schema)\n",
    "except Exception as err:\n",
    "    print(\"Table not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's recall that in the previous notebook, the table we created has the following tableSchema and indexSchema:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```python\n",
    "tabSchema = TableSchema(tabName, StructType([\n",
    "    StructField(\"deviceID\", IntegerType(), nullable = False),\n",
    "    StructField(\"sensorID\", IntegerType(), nullable = False),\n",
    "    StructField(\"ts\", LongType(), nullable = False),\n",
    "    StructField(\"ambient_temp\", DoubleType(), nullable = False),\n",
    "    StructField(\"power\", DoubleType(), nullable = False),\n",
    "    StructField(\"temperature\", DoubleType(), nullable = False)\n",
    "    ]),\n",
    "    sharding_columns = [\"deviceID\", \"sensorID\"],\n",
    "    pk_columns = [\"deviceID\", \"sensorID\", \"ts\"]\n",
    "                       )\n",
    "\n",
    "indexSchema = IndexSpecification(\n",
    "          index_name=tabName + \"Index\",\n",
    "          table_schema=tabSchema,\n",
    "          equal_columns = [\"deviceID\", \"sensorID\"],\n",
    "          sort_columns = [\n",
    "            SortSpecification(\"ts\", ColumnOrder.DESCENDING_NULLS_LAST)],\n",
    "          include_columns = [\"temperature\"]\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Best Practices for efficient queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the next cell we create a lazily evaluated \"view\" that we can then use like a hive table in Spark SQL, but this is only evaluated when we actually run or cache query results. We are calling this view \"readings\" and that is how we will refer to it in the queries below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tab.createOrReplaceTempView(\"readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"SELECT count(*) FROM readings\"\n",
    "print(\"{}\\nRunning query in Event Store...\".format(query))\n",
    "df_data = eventSession.sql(query)\n",
    "df_data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's have a look at the record structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM readings LIMIT 1\"\n",
    "print(\"{}\\nRunning query in Event Store...\".format(query))\n",
    "df_data = eventSession.sql(query)\n",
    "df_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = \"SELECT MIN(ts), MAX(ts) FROM readings\"\n",
    "print(\"{}\\nRunning query in Event Store...\".format(query))\n",
    "df_data = eventSession.sql(query)\n",
    "df_data.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Optimal query through the index\n",
    "\n",
    "- Index queries will significantly reduce amount of data that needs to be scanned for results. \n",
    "    - Indexes in IBM Db2 Event Store are formed asynchronously to avoid insert latency. \n",
    "    - They are stored as a Log Structured Merge (LSM) Tree.\n",
    "    - The index is formed by \"runs\", which include sequences of sorted keys. These runs are written to disk during “Share” processing.\n",
    "    - These index runs are merged together over time to improve scan and I/O efficiency.\n",
    "\n",
    "- For an optimal query performance you must specify equality on all the equal_columns in the index and a range on the sort column in the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For example, in the following query we are retrieving all the values in the range of dates for a specific device and sensor, where both the `deviceID` and `sensorID` are in the equal_columns definition for the index schema and the `ts` column is the sort column for the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "index_query = \"SELECT ts, temperature  FROM readings where deviceID=1 and sensorID=12 and ts >1541021271619 and ts < 1541043671128 order by ts\"\n",
    "print(\"{}\\nCreating a dataframe for the query ...\".format(index_query))\n",
    "df_index_query = eventSession.sql(index_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then the following cell runs the query and caches the results. Note that this caching is for demostration purposes, to show the time it takes to run the query and cache the results in memory within Spark. This caching is recommended when you are going to do additional processing on this cached data, as the query against IBM Db2 Event Store is only run once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_index_query.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally the results from the cached data can be visualized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_index_query.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sub-optimal query\n",
    "\n",
    "This next query shows a sub-optimal query that only specifies equality in one of the equal_columns in the index schema, and for this reason ends up doing a full scan of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fullscan_query = \"SELECT count(*)  FROM readings where sensorID = 7\"\n",
    "print(\"{}\\nCreating a dataframe for the query...\".format(fullscan_query))\n",
    "df_fullscan_query = eventSession.sql(fullscan_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_fullscan_query.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_fullscan_query.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Accessing multiple sensorIDs optimally\n",
    "\n",
    "The easiest way to write a query that needs to retrieve multiple values in the equal_columns in the index schema is by using an *In-List*. With this, you can get optimal index access across multiple sensorID's. \n",
    "\n",
    "In this example we specify equality for a specific deviceID, and an In-List for the four sensors we are trying to retrieve. To limit the number of records we are returning we also include a range of timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inlist_query = \"SELECT deviceID, sensorID, ts  FROM readings where deviceID=1 and sensorID in (1, 5, 7, 12) and ts >1541021271619 and ts < 1541043671128 order by ts\"\n",
    "print(\"{}\\nCreating a dataframe for the query...\".format(inlist_query))\n",
    "df_inlist_query = eventSession.sql(inlist_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_inlist_query.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_inlist_query.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exploiting the synopsis table:  Advanced medium weight queries\n",
    "\n",
    "- Event Store tables include a synopsis table which summarizes the minimum/maximum values of the data for each range of rows in the synopsis table. \n",
    "\n",
    "     - It contains one range for every 1000 rows.\n",
    "     - It is stored in a separate internal table in the shared storage layer.\n",
    "     - It is parquet compressed to minimize footprint.\n",
    "     - For highly selective queries, it can improve performance by up to 1000x.\n",
    "\n",
    "- Using an equality or a range predicate on a clustered field (e.g. the 'ts' column in our case because the data is inserted into the table in order) is faster than doing a full scan as it should be able to exploit the synopsis table, but this will be slower than an optimal index scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "synopsis_query = \"SELECT deviceID, sensorID, ts FROM readings where ts >1541021271619 and ts < 1541043671128 order by ts\"\n",
    "print(\"{}\\nCreating a dataframe for the query...\".format(synopsis_query))\n",
    "df_synopsis_query = eventSession.sql(synopsis_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_synopsis_query.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_synopsis_query.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Summary\n",
    "This demo introduced you to the best practices querying the table stored in IBM Db2 Event Store database.\n",
    "\n",
    "## Next Step\n",
    "`\"Event_Store_Data_Analytics.ipynb\"` will show you how to perform data analytics with IBM Db2 Event Store with multiple scientific tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with Watson Studio Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
