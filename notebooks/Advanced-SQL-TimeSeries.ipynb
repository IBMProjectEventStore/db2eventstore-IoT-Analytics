{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisite:\n",
    "Before running this notebook, you will have to:\n",
    "1. download the csv file named `dht_1k.csv` and `sds_1k.csv`  \n",
    "stored under https://github.com/IBMProjectEventStore/db2eventstore-IoT-Analytics/tree/master/data.\n",
    "2. Go to the `Project tab` and load both above mentioned csv files into the current project as dataset.\n",
    "----\n",
    "**Note: This Notebook can only run in Python version >= 3.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from eventstore.oltp import EventContext\n",
    "from eventstore.sql import EventSession\n",
    "from eventstore.common import ConfigurationReader\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ConfigurationReader.setEventUser(\"\")\n",
    "ConfigurationReader.setEventPassword(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"EventStore SQL in Python\").getOrCreate()\n",
    "eventSession = EventSession(sparkSession.sparkContext, \"EVENTDB\")\n",
    "eventSession.set_query_read_option(\"SnapshotNow\")\n",
    "eventSession._jvm.org.apache.spark.sql.types.SqlTimeSeries.register(eventSession._jsparkSession)\n",
    "eventSession.open_database()\n",
    "ctx = EventContext.get_event_context(\"EVENTDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from eventstore.catalog import TableSchema\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "table_names = ctx.get_names_of_tables()\n",
    "for idx, name in enumerate(table_names):\n",
    "    print(idx, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def datetime_converter(datetime_string):\n",
    "    # (1) Convert to datetime format\n",
    "    utc_time = datetime.strptime(datetime_string.split('.000Z')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    return int((utc_time - datetime(1970, 1, 1)).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## create table and loading data for DHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define table schema to be created\n",
    "with EventContext.get_event_context(\"EVENTDB\") as ctx:\n",
    "    schema = StructType([\n",
    "        StructField(\"sensor_id\", IntegerType(), nullable = False),\n",
    "        StructField(\"timestamp\", IntegerType(), nullable = False),\n",
    "        StructField(\"location\", IntegerType(), nullable = False),\n",
    "        StructField(\"humidity\", FloatType(), nullable = True)\n",
    "    ])  \n",
    "    table_schema = TableSchema(\"dht_table\", schema,\n",
    "                                sharding_columns=[\"sensor_id\"],\n",
    "                                pk_columns=[\"sensor_id\",\"timestamp\",\"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try create table if not exist\n",
    "# try:\n",
    "#     ctx.drop_table(\"DHT_TABLE\")\n",
    "# except Exception as error:\n",
    "#     print(error)\n",
    "try:\n",
    "    ctx.create_table(table_schema)\n",
    "except Exception as error:\n",
    "    pass\n",
    "    \n",
    "table_names = ctx.get_names_of_tables()\n",
    "for idx, name in enumerate(table_names):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dht_table = eventSession.load_event_table(\"dht_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ingest data into table\n",
    "import os\n",
    "resolved_table_schema = ctx.get_table(\"dht_table\")\n",
    "print(resolved_table_schema)\n",
    "with open(os.environ['DSX_PROJECT_DIR']+'/datasets/dht_1k.csv') as f:\n",
    "    f.readline()\n",
    "    content = f.readlines()\n",
    "content = [l.split(\",\") for l in content]\n",
    "batch = [dict(sensor_id=int(c[5]), timestamp=datetime_converter(c[7]), location=int(c[0]), humidity=float(c[2])) for c in content]\n",
    "ctx.batch_insert(resolved_table_schema, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# verify ingested result\n",
    "dht_table = eventSession.load_event_table(\"dht_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dht_table.createOrReplaceTempView(\"dht_raw_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eventSession.sql(\"select count(*) from dht_raw_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## create table and loading data for SDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with EventContext.get_event_context(\"EVENTDB\") as ctx:\n",
    "    schema = StructType([\n",
    "        StructField(\"sensor_id\", IntegerType(), nullable = False),\n",
    "        StructField(\"timestamp\", LongType(), nullable = False),\n",
    "        StructField(\"location\", IntegerType(), nullable = False),\n",
    "        StructField(\"p_1\", DoubleType(), nullable = True)\n",
    "    ])  \n",
    "    table_schema = TableSchema(\"sds_table\", schema,\n",
    "                                sharding_columns=[\"sensor_id\"],\n",
    "                                pk_columns=[\"sensor_id\",\"timestamp\",\"location\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     ctx.drop_table(\"SDS_TABLE\")\n",
    "# except Exception as error:\n",
    "#     print(error)\n",
    "try:\n",
    "    ctx.create_table(table_schema)\n",
    "except Exception as error:\n",
    "    print(\"Table not created.\")\n",
    "table_names = ctx.get_names_of_tables()\n",
    "for idx, name in enumerate(table_names):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sds_table = eventSession.load_event_table(\"sds_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with EventContext.get_event_context(\"EVENTDB\") as ctx:\n",
    "    resolved_table_schema = ctx.get_table(\"sds_table\")\n",
    "    with open(os.environ['DSX_PROJECT_DIR']+'/datasets/sds_1k.csv') as f:\n",
    "        f.readline()\n",
    "        content = f.readlines()\n",
    "    content = [l.split(\",\") for l in content]\n",
    "    batch = [dict(sensor_id=int(c[5]), timestamp=datetime_converter(c[7]), location=int(c[0]), p_1=float(c[2])) for c in content if c[2] != \"\"]\n",
    "    ctx.batch_insert(resolved_table_schema, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sds_table=eventSession.load_event_table(\"sds_table\")\n",
    "sds_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sds_table.createOrReplaceTempView(\"sds_raw_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eventSession.sql(\"select * from sds_raw_table\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sql=\"SELECT count(*) FROM dht_raw_table\"\n",
    "eventSession.sql(sql).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Alignment of multiple time-series from different IoT sensors\n",
    "\n",
    "The below example shows how one can align two timeseries using an inner join with the nearest timestamp. The problem arises when two sensors are generating timestamps with clocks that are not fully synchronized. This can occur regularly and is common in IoT scenarios.\n",
    "\n",
    "In what follows, we will show an example on the OK Lab data that considers the timeseries generated by the SDS (particulate matter) sensors and the DHT (humidity/temperature) sensors. The manual of SDS states that the SDS values are valid only when the DHT sensor measures humidity as < 70%. Realizing this restraint requires the data from the 2 sensors to be synchronized temporally. We note first that traditional join style would be to look for exact timestamps, which will not work in this case as the sensors are coming from two different devices (with the same location). One approach would be to use a standard SQL statement such as:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "    humidity, \n",
    "    p_1\n",
    "FROM sds_raw_table STORED AS PARQUET sds011, dht_raw_table STORED AS PARQUET dht22\n",
    "AND dht22.humidity <= 70\n",
    "AND ((sds011.timestamp - INTERVAL 10 SECONDS) < dht22.timestamp) \n",
    "AND (dht22.timestamp < (sds011.timestamp + INTERVAL 10 SECONDS))\n",
    "```\n",
    "\n",
    "However, if executed as it stands, it can take several hours to complete given that it results in a traditional Cartesian Join.\n",
    "\n",
    "In our example, we will down-select the data using DB2 Eventstore into two dataframes (sds and dht). Our approach then is to do a clever join that takes windowing into account and not do a full Cartesian join. \n",
    "\n",
    "In what follows, we we will show how using time-series capabilities of DB2-Eventstore can be used to address this problem of unaligned sensors. Although this is one application that time-series for DB2-eventstore covers, time-series capabilites are not limited to just this one use case as we have functions to handle simple statistical methods (fft, avg, percentile, etc.), time-series distance metrics (DL, DTW, SBD, etc.), sophisticated forms of segmentation (time-based, anchor-base, marker-base, record-based, etc.), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Initial exploration on data\n",
    "\n",
    "In many cases, the first step to doing any Time-Series analysis is to learn about your time-series. To do so, we use what is called a describe. This will provide a rich set of metrics (avg, percentiles, timing-statistics, etc.) over a time-series such that a user can have some knowledge of the time-series they are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stmt = \"\"\"SELECT location, TS_DESCRIBE(ts) FROM ( SELECT location,TIME_SERIES(timestamp, humidity) AS ts FROM dht_raw_table where humidity < 70 GROUP BY location)\n",
    "\"\"\"\n",
    "\n",
    "df = eventSession.sql(stmt).toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Description of performing a time-series sql temporal align\n",
    "\n",
    "This query has a few main things to consider:\n",
    "\n",
    "### Creating your time series\n",
    "\n",
    "first a time series must be created\n",
    "\n",
    "#### sds time series\n",
    "\n",
    "```sql\n",
    "SELECT location, D_TIME_SERIES(timestamp, p_1) AS sds FROM sds_raw_table GROUP BY location\n",
    "```\n",
    "\n",
    "#### dht time series\n",
    "\n",
    "```sql\n",
    "SELECT location, D_TIME_SERIES(timestamp, humidity) AS dht FROM dht_raw_table GROUP BY location\n",
    "```\n",
    "\n",
    "### Performing full temporal align\n",
    "\n",
    "Performing a full temporal align requires 2 parameters:\n",
    "\n",
    "- The left Time Series\n",
    "- The right Time Series\n",
    "\n",
    "Once given, the returned output will be 2 columns (the 2 aligned time series) as **left_column_aligned** and **right_column_aligned**\n",
    "\n",
    "*Note: With this method, all missing values will be replaced with null*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stmt = \"\"\"\n",
    "    SELECT sds_table.location, TS_FULL_ALIGN(dht, sds, TS_INTERPOLATOR_NEAREST(-1.0)) FROM \n",
    "        (SELECT location, TIME_SERIES(timestamp, humidity) AS dht FROM dht_raw_table where humidity < 70 GROUP BY location) AS dht_table\n",
    "        INNER JOIN\n",
    "        (SELECT location, TIME_SERIES(timestamp, p_1) AS sds FROM sds_raw_table GROUP BY location) AS sds_table\n",
    "        ON dht_table.location = sds_table.location\n",
    "\"\"\"\n",
    "df = eventSession.sql(stmt)\n",
    "df.show()\n",
    "df.count()\n",
    "eventSession.sql(stmt).createOrReplaceTempView(\"dht_sds_ts_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Display the aligned TimeSeries table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eventSession.sql(\"select count(*) from dht_sds_ts_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Interpolate missing values after alignment\n",
    "\n",
    "Because in IoT use cases, sensors tend to be clocked at different rates, it's important to properly fill values where they don't exist in the data. Just because a value is not in our data, does not mean it did not exist. To approximate the missing value, we can provide an interpolator as simple as nearest, next, prev, but as sophisticated as linear interpolation or cubic spline interpolation. In the following example, we will fill all missing values based on a nearest interpolation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eventSession.sql(\"SELECT location, TS_FILLNA(dht_aligned,TS_INTERPOLATOR_NEAREST(-1.0)) as ts FROM dht_sds_ts_table\").createOrReplaceTempView(\"dht_no_nulls\")\n",
    "eventSession.sql(\"SELECT location, TS_FILLNA(sds_aligned,TS_INTERPOLATOR_NEAREST(-1.0)) as ts FROM dht_sds_ts_table\").createOrReplaceTempView(\"sds_no_nulls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Converting Time-Series data to tabular data\n",
    "\n",
    "Once all Time-Series analysis has been done, because Time-Series types are not directly ingestable, a user may want to display there data in a tabular format to prepare for graphing or performing further analysis. The following is how would could convert that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eventSession.sql(\"SELECT location, TS_EXPLODE(ts) FROM dht_no_nulls\").createOrReplaceTempView(\"dht_exploded\")\n",
    "eventSession.sql(\"SELECT location, TS_EXPLODE(ts) FROM sds_no_nulls\").createOrReplaceTempView(\"sds_exploded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eventSession.sql(\"select * from dht_exploded\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eventSession.sql(\"select * from sds_exploded\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Joining the tabular data\n",
    "\n",
    "Lastly, we will perform a classical join on the location and time_tick for humidity and coarse particulate matter data to properly display the aligned values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stmt = \"\"\"\n",
    "    select dht_exploded.location, dht_exploded.ts_timeTick as timestamp, dht_exploded.ts_value as humidity, sds_exploded.ts_value as p_1 FROM\n",
    "        dht_exploded\n",
    "        INNER JOIN\n",
    "        sds_exploded\n",
    "        ON dht_exploded.location=sds_exploded.location and dht_exploded.ts_timeTick=sds_exploded.ts_timeTick\n",
    "\"\"\"\n",
    "df = eventSession.sql(stmt)\n",
    "df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Timing for Time-Series SQL\n",
    "This section shows how long it takes to run each time-series SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "stmt = \"SELECT TIME_SERIES(timestamp, humidity) AS dht FROM dht_raw_table\"\n",
    "\n",
    "df = eventSession.sql(stmt)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"table_a\")\n",
    "start = time.time()\n",
    "df.show()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "total_time = end - start\n",
    "print(\"ingestion time: \" + str(total_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stmt = \"SELECT TS_RESAMPLE(dht, 3600, TS_INTERPOLATOR_NEAREST(0.0)) as dht_interp from table_a\"\n",
    "start = time.time()\n",
    "eventSession.sql(stmt).show()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "total_time = end - start\n",
    "print(\"ingestion time: \" + str(total_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stmt = \"select count(*) from (SELECT TS_EXPLODE(dht) from table_a)\"\n",
    "start = time.time()\n",
    "eventSession.sql(stmt).show()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "total_time = end - start\n",
    "print(\"ingestion time: \" + str(total_time) + \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with Watson Studio Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
