{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisite:\n",
    "Before running this notebook, you will have to:\n",
    "1. download the csv file named `dht_1k.csv` and `sds_1k.csv`  \n",
    "stored under https://github.com/IBMProjectEventStore/db2eventstore-IoT-Analytics/tree/master/data.\n",
    "2. Go to the `Project tab` and load both above mentioned csv files into the current project as dataset.\n",
    "----\n",
    "**Note: This Notebook can only run in Python version >= 3.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from eventstore.oltp import EventContext\n",
    "from eventstore.sql import EventSession\n",
    "from eventstore.common import ConfigurationReader\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ConfigurationReader.setEventUser(\"\")\n",
    "ConfigurationReader.setEventPassword(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"EventStore SQL in Python\").getOrCreate()\n",
    "eventSession = EventSession(sparkSession.sparkContext, \"EVENTDB\")\n",
    "eventSession.set_query_read_option(\"SnapshotNow\")\n",
    "eventSession._jvm.org.apache.spark.sql.types.SqlTimeSeries.register(eventSession._jsparkSession)\n",
    "eventSession.open_database()\n",
    "ctx = EventContext.get_event_context(\"EVENTDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from eventstore.catalog import TableSchema\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def datetime_converter(datetime_string):\n",
    "    # (1) Convert to datetime format\n",
    "    utc_time = datetime.strptime(datetime_string.split('.000Z')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    return int((utc_time - datetime(1970, 1, 1)).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = ctx.get_names_of_tables()\n",
    "for idx, name in enumerate(table_names):\n",
    "    print(idx, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create(df, key_col, ts_col, val_col, new_key_name=\"joined_primary_keys\", time_series_name=None):\n",
    "    \"\"\"\n",
    "    Highly efficient algorithm that creates a time series from the Spark Dataframe.\n",
    "    ---\n",
    "    @param df: Spark Dataframe : Containing input columns for time series creation\n",
    "    @key_col: List[String] : List of column name strings of primary key for the time series creation\n",
    "    @ts_col: String : Column name of timestamp\n",
    "    @val_col: String : Column name of value\n",
    "    @new_key_name: String : Column name of the joined primary key column to be created.\n",
    "    @time_series_name: String : [Default: <val_col>_time_series] Column name of the time series column to be created.\n",
    "    return: [Spark Dataframe] Spark df containing 2 columns: key column and time series column\n",
    "    ---\n",
    "    Example:\n",
    "    ts_df = create(raw_table_with_dates, [\"SATID\",\"PKID\",\"DATE\"], \"TIMESTAMP\", \"READING\")\n",
    "    \"\"\"\n",
    "    from pyspark.sql import DataFrame\n",
    "    from pyspark.sql.functions import concat, col, lit\n",
    "    ts_column_name = val_col + \"_time_series\"\n",
    "    df = df.withColumn(new_key_name, concat(*key_col))\n",
    "    ts_df = DataFrame(\n",
    "        df.sql_ctx._jvm.com.ibm.research.time_series.spark_timeseries_sql.utils.api.java.TimeSeriesDataFrame.create(\n",
    "            df._jdf,\n",
    "            new_key_name,\n",
    "            ts_col,\n",
    "            val_col\n",
    "        ),\n",
    "        df.sql_ctx\n",
    "    )\n",
    "    if time_series_name:\n",
    "        ts_df = ts_df.withColumnRenamed(ts_column_name, time_series_name)\n",
    "    return ts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showPartitionInfo(df):\n",
    "    # show partition number and number of records in a partition in the given Spark dataframe\n",
    "    #@df: Spark DataFrame\n",
    "    print(\"- number of partitions prior to time series (after loading table): \",df.rdd.getNumPartitions())\n",
    "    print(\"- partition sizes prior to time series (after loading table): \", df.rdd.mapPartitions(lambda s: iter([sum(1 for _ in s)])).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- **Create DHT table and loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table schema to be created\n",
    "with EventContext.get_event_context(\"EVENTDB\") as ctx:\n",
    "    schema = StructType([\n",
    "        StructField(\"sensor_id\", IntegerType(), nullable = False),\n",
    "        StructField(\"timestamp\", IntegerType(), nullable = False),\n",
    "        StructField(\"location\", IntegerType(), nullable = False),\n",
    "        StructField(\"humidity\", FloatType(), nullable = True)\n",
    "    ])  \n",
    "    table_schema = TableSchema(\"dht_table\", schema,\n",
    "                                sharding_columns=[\"sensor_id\"],\n",
    "                                pk_columns=[\"sensor_id\",\"timestamp\",\"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try create table if not exist\n",
    "# try:\n",
    "#     ctx.drop_table(\"DHT_TABLE\")\n",
    "# except Exception as error:\n",
    "#     print(error)\n",
    "try:\n",
    "    ctx.create_table(table_schema)\n",
    "except Exception as error:\n",
    "    pass\n",
    "    \n",
    "table_names = ctx.get_names_of_tables()\n",
    "for idx, name in enumerate(table_names):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dht_table = eventSession.load_event_table(\"dht_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dht_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest data into table\n",
    "import os\n",
    "resolved_table_schema = ctx.get_table(\"dht_table\")\n",
    "print(resolved_table_schema)\n",
    "with open(os.environ['DSX_PROJECT_DIR']+'/datasets/dht_1k.csv') as f:\n",
    "    f.readline()\n",
    "    content = f.readlines()\n",
    "content = [l.split(\",\") for l in content]\n",
    "batch = [dict(sensor_id=int(c[5]), timestamp=datetime_converter(c[7]), location=int(c[0]), humidity=float(c[2])) for c in content]\n",
    "ctx.batch_insert(resolved_table_schema, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# verify ingested result\n",
    "dht_table = eventSession.load_event_table(\"dht_table\")\n",
    "dht_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPartitionInfo(dht_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dht_table.createOrReplaceTempView(\"dht_raw_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventSession.sql(\"select count(*) from dht_raw_table\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- **Create SDS table and loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with EventContext.get_event_context(\"EVENTDB\") as ctx:\n",
    "    schema = StructType([\n",
    "        StructField(\"sensor_id\", IntegerType(), nullable = False),\n",
    "        StructField(\"timestamp\", LongType(), nullable = False),\n",
    "        StructField(\"location\", IntegerType(), nullable = False),\n",
    "        StructField(\"p_1\", DoubleType(), nullable = True)\n",
    "    ])  \n",
    "    table_schema = TableSchema(\"sds_table\", schema,\n",
    "                                sharding_columns=[\"sensor_id\"],\n",
    "                                pk_columns=[\"sensor_id\",\"timestamp\",\"location\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     ctx.drop_table(\"SDS_TABLE\")\n",
    "# except Exception as error:\n",
    "#     print(error)\n",
    "try:\n",
    "    ctx.create_table(table_schema)\n",
    "except Exception as error:\n",
    "    print(\"Table not created.\")\n",
    "table_names = ctx.get_names_of_tables()\n",
    "for idx, name in enumerate(table_names):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sds_table = eventSession.load_event_table(\"sds_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with EventContext.get_event_context(\"EVENTDB\") as ctx:\n",
    "    resolved_table_schema = ctx.get_table(\"sds_table\")\n",
    "    with open(os.environ['DSX_PROJECT_DIR']+'/datasets/sds_1k.csv') as f:\n",
    "        f.readline()\n",
    "        content = f.readlines()\n",
    "    content = [l.split(\",\") for l in content]\n",
    "    batch = [dict(sensor_id=int(c[5]), timestamp=datetime_converter(c[7]), location=int(c[0]), p_1=float(c[2])) for c in content if c[2] != \"\"]\n",
    "    ctx.batch_insert(resolved_table_schema, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_table=eventSession.load_event_table(\"sds_table\")\n",
    "sds_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPartitionInfo(sds_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sds_table.createOrReplaceTempView(\"sds_raw_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventSession.sql(\"select * from sds_raw_table\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sql=\"SELECT count(*) FROM dht_raw_table\"\n",
    "eventSession.sql(sql).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Repartitioning**\n",
    "\n",
    "Optimize parallelism by repartitioning\n",
    "Note that when the query is pushed down to Db2 Event Store and the data is retrieved, the data will be received by Spark as one single partitioned data frame. It's necessary for the user to explicitly repartition the dataframe.\n",
    "It's suggested that one partition is created for each CPU core in the Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dht_table\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPartitionInfo(dht_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dht_table = dht_table.repartition(48)\n",
    "showPartitionInfo(dht_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sds_table\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPartitionInfo(sds_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_table = sds_table.repartition(48)\n",
    "showPartitionInfo(sds_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment of multiple time-series from different IoT sensors\n",
    "\n",
    "The below example shows how one can align two timeseries using an inner join with the nearest timestamp. The problem arises when two sensors are generating timestamps with clocks that are not fully synchronized. This can occur regularly and is common in IoT scenarios.\n",
    "\n",
    "In what follows, we will show an example on the OK Lab data that considers the timeseries generated by the SDS (particulate matter) sensors and the DHT (humidity/temperature) sensors. The manual of SDS states that the SDS values are valid only when the DHT sensor measures humidity as < 70%. Realizing this restraint requires the data from the 2 sensors to be synchronized temporally. We note first that traditional join style would be to look for exact timestamps, which will not work in this case as the sensors are coming from two different devices (with the same location). One approach would be to use a standard SQL statement such as:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "    humidity, \n",
    "    p_1\n",
    "FROM sds_raw_table STORED AS PARQUET sds011, dht_raw_table STORED AS PARQUET dht22\n",
    "AND dht22.humidity <= 70\n",
    "AND ((sds011.timestamp - INTERVAL 10 SECONDS) < dht22.timestamp) \n",
    "AND (dht22.timestamp < (sds011.timestamp + INTERVAL 10 SECONDS))\n",
    "```\n",
    "\n",
    "However, if executed as it stands, it can take several hours to complete given that it results in a traditional Cartesian Join.\n",
    "\n",
    "In our example, we will down-select the data using DB2 Eventstore into two dataframes (sds and dht). Our approach then is to do a clever join that takes windowing into account and not do a full Cartesian join. \n",
    "\n",
    "In what follows, we we will show how using time-series capabilities of DB2-Eventstore can be used to address this problem of unaligned sensors. Although this is one application that time-series for DB2-eventstore covers, time-series capabilites are not limited to just this one use case as we have functions to handle simple statistical methods (fft, avg, percentile, etc.), time-series distance metrics (DL, DTW, SBD, etc.), sophisticated forms of segmentation (time-based, anchor-base, marker-base, record-based, etc.), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating your time series\n",
    "\n",
    "First a time series must be created. IBM Db2 Event Store provides two ways of creating a time series, namely Spark UDAF SQL function and `create` function.\n",
    "\n",
    "1/ **Spark User Defined Aggregate Function (UDAF) : `TIME_SERIES`**\n",
    "\n",
    "Spark UDAF goes through each row of the dataframe, and creates a new time series by aggregating the previous rows and current row.\n",
    "Because the Spark RDD is immutable, multiple intermediate RDDs will be created.\n",
    "\n",
    "so for instance if you have 3 rows: [1] | [2] | [3], it will look as such [1] … [1] + [2] = [1,2] … [1,2] + [3] = [1,2,3]. Thus 5 intermediate rdds are created: [1] [2] [3] [1,2] [1,2,3].\n",
    "\n",
    "Example: \n",
    "```sql\n",
    "stmt = \"SELECT location, TIME_SERIES(timestamp, humidity) AS dht FROM dht_raw_table where humidity < 70 GROUP BY location\"\n",
    "```\n",
    "\n",
    "2/ **create function:**\n",
    "\n",
    "The create function simply group the given dataframe by key, and create one time series for each key at once. The performance advantage of the create function will be increasingly obvious with the larger dataframe and time series size.\n",
    "\n",
    "Example: \n",
    "```python\n",
    "create(dht_table, [\"location\"], \"timestamp\", \"HUMIDITY\", \"LOCATION\")\n",
    "```\n",
    " \n",
    "In this notebook, we will use the `create` function, instead of the Spark UDAF function, to create the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create dht time series\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dht_ts = create(dht_table, [\"location\"], \"timestamp\", \"HUMIDITY\", \"LOCATION\", \"dht\")\n",
    "dht_ts.createOrReplaceTempView(\"dht_ts_table\")\n",
    "dht_ts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create sds time series\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sds_ts = create(sds_table, [\"location\"], \"timestamp\", \"P_1\", \"LOCATION\", \"sds\")\n",
    "sds_ts.createOrReplaceTempView(\"sds_ts_table\")\n",
    "sds_ts.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial exploration on data\n",
    "\n",
    "In many cases, the first step to doing any Time-Series analysis is to learn about your time-series. To do so, we use what is called a describe. This will provide a rich set of metrics (avg, percentiles, timing-statistics, etc.) over a time-series such that a user can have some knowledge of the time-series they are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Exploration on the DHT (humidity/temperature) sensors data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stmt = \"\"\"\n",
    "SELECT location, TS_DESCRIBE(dht) FROM dht_ts_table\n",
    "\"\"\"\n",
    "\n",
    "eventSession.sql(stmt).toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"|\n",
    "Exploration on SDS (particulate matter) sensors data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stmt = \"\"\"\n",
    "SELECT location, TS_DESCRIBE(sds) FROM sds_ts_table\n",
    "\"\"\"\n",
    "\n",
    "eventSession.sql(stmt).toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of performing a time-series sql temporal align\n",
    "\n",
    "This query has a few main things to consider:\n",
    "\n",
    "### Performing full temporal align\n",
    "\n",
    "Performing a full temporal align requires 2 parameters:\n",
    "\n",
    "- The left Time Series\n",
    "- The right Time Series\n",
    "\n",
    "Once given, the returned output will be 2 columns (the 2 aligned time series) as **left_column_aligned** and **right_column_aligned**\n",
    "\n",
    "*Note: With this method, all missing values will be replaced with null*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stmt = \"\"\"\n",
    "    SELECT sds_ts_table.location, TS_FULL_ALIGN(dht, sds, TS_INTERPOLATOR_NEAREST(-1.0)) FROM \n",
    "        dht_ts_table\n",
    "        INNER JOIN\n",
    "        sds_ts_table\n",
    "        ON dht_ts_table.location = sds_ts_table.location\n",
    "\"\"\"\n",
    "df = eventSession.sql(stmt)\n",
    "df.show(5)\n",
    "df.count()\n",
    "eventSession.sql(stmt).createOrReplaceTempView(\"dht_sds_ts_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the aligned TimeSeries table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "eventSession.sql(\"select count(*) from dht_sds_ts_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate missing values after alignment\n",
    "\n",
    "Because in IoT use cases, sensors tend to be clocked at different rates, it's important to properly fill values where they don't exist in the data. Just because a value is not in our data, does not mean it did not exist. To approximate the missing value, we can provide an interpolator as simple as nearest, next, prev, but as sophisticated as linear interpolation or cubic spline interpolation. In the following example, we will fill all missing values based on a nearest interpolation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eventSession.sql(\"SELECT location, TS_FILLNA(dht_aligned,TS_INTERPOLATOR_NEAREST(-1.0)) as ts FROM dht_sds_ts_table\").createOrReplaceTempView(\"dht_no_nulls\")\n",
    "eventSession.sql(\"SELECT location, TS_FILLNA(sds_aligned,TS_INTERPOLATOR_NEAREST(-1.0)) as ts FROM dht_sds_ts_table\").createOrReplaceTempView(\"sds_no_nulls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Time-Series data to tabular data\n",
    "\n",
    "Once all Time-Series analysis has been done, because Time-Series types are not directly ingestable, a user may want to display there data in a tabular format to prepare for graphing or performing further analysis. The following is how would could convert that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventSession.sql(\"SELECT location, TS_EXPLODE(ts) FROM dht_no_nulls\").createOrReplaceTempView(\"dht_exploded\")\n",
    "eventSession.sql(\"SELECT location, TS_EXPLODE(ts) FROM sds_no_nulls\").createOrReplaceTempView(\"sds_exploded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventSession.sql(\"select * from dht_exploded\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventSession.sql(\"select * from sds_exploded\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining the tabular data\n",
    "\n",
    "Lastly, we will perform a classical join on the location and time_tick for humidity and coarse particulate matter data to properly display the aligned values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stmt = \"\"\"\n",
    "    select dht_exploded.location, dht_exploded.ts_timeTick as timestamp, dht_exploded.ts_value as humidity, sds_exploded.ts_value as p_1 FROM\n",
    "        dht_exploded\n",
    "        INNER JOIN\n",
    "        sds_exploded\n",
    "        ON dht_exploded.location=sds_exploded.location and dht_exploded.ts_timeTick=sds_exploded.ts_timeTick\n",
    "\"\"\"\n",
    "df = eventSession.sql(stmt)\n",
    "df.show()\n",
    "df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
